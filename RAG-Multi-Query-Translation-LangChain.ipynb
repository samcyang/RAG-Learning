{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "RAG - Multi-Query Query translation\n",
    "\n",
    "Multi-Query translation will create similar queries from original question. Based those\n",
    "queris, this algorithm will retieve the relevent information/document as part of the\n",
    "prompt context before the final Q&A\n",
    "\n",
    "step_to_step_query, at end, is based on the LangChain Rag youtube tutorial below, the code from the tutorial are \n",
    "listed in the first half of the python notebook. \n",
    "\n",
    "This function follows the same logic from the tutorial but without the abstract of Chain/Pipeline. This approach\n",
    "it helps me to understand the llm processes and also can improve the prompt preparation.\n",
    "Of course, the langsmith is a great tool as well. \n",
    "\n",
    "\n",
    "Input arguments to the step_to_step_fusion are\n",
    "1. Document: the resume as the source of information\n",
    "2. Quesition: question string\n",
    "\n",
    "\n",
    "Please review the RAG tutorial from Langchain in details (part 5 - multi-query query translation)\n",
    "https://www.youtube.com/watch?v=JChPi0CRnDY\n",
    "\n",
    "'''\n",
    "#RAG-- Common Rag\n",
    "# Query Translation:   Multi-Query, Fusion and Decompsoition \n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "#if (GCP_PROJECT_ID == None): print (\"Not set\")\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "LANGCHAIN_API_KEY = os.getenv('LANGCHAIN_API_KEY')\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'false'  #true for trace\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"RAG - Multi-Query-Translation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RAG-- Common Rag\n",
    "# Query Translation:   Multi-Query, Fusion and Decompsoition \n",
    "from langchain import hub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "\n",
    "#Using CharaterTextSplitter may have better result than RecursiveCharacterTextSplitter\n",
    "#text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=40)\n",
    "text_splitter = CharacterTextSplitter()\n",
    "#Chunk size has no effects on CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Example: Document retriving from Web ####\n",
    "'''\n",
    "# Load Documents from Web\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "doc = loader.load()\n",
    "docs = text_splitter.split_documents(doc)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RAG-- Common Rag\n",
    "# Query Translation:   Multi-Query, Fusion and Decompsoition \n",
    "\n",
    "from PyPDF2 import PdfReader\n",
    "def extract_pdf_text(file_path):\n",
    "    pdf_file = PdfReader(file_path)\n",
    "    text_data = ''\n",
    "    for pg in pdf_file.pages:\n",
    "        text_data += pg.extract_text()\n",
    "    return text_data\n",
    "resume_name = \".\\\\docs\\\\samcyangResume_Gen123.pdf\"\n",
    "pdf_text = extract_pdf_text(resume_name)\n",
    "pdf_texts = text_splitter.split_text(pdf_text)\n",
    "split_docs = text_splitter.create_documents(pdf_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieving\n",
    "# RAG-- Common Rag\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=split_docs, \n",
    "                                    embedding=OpenAIEmbeddings(model=\"text-embedding-3-small\"))\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4}, max_tokens_limit=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, Multi-Query or Query-Translation\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "question = \"List all the companies Sam have worked for, please also list the year he worked for those companies\"\n",
    "# Multi Query: Different Perspectives\n",
    "#question = \"Please provide summary of Sam work experience\"\n",
    "template = \"\"\"You are an AI language model assistant. Your task is to generate 5\n",
    "different versions of the given user question to retrieve relevant documents from a vector \n",
    "database. By generating multiple perspectives on the user question, your goal is to help\n",
    "the user overcome some of the limitations of the distance-based similarity search. \n",
    "Provide these alternative questions separated by newlines.  Original question: {question}\"\"\"\n",
    "'''\n",
    "template = \"\"\"A resume may include 0. The file name is listed in the first line,\n",
    "1. The resume contains the name, the phone number and the address of a person, \\n\n",
    "2. The resume contains the degree and attending university and years of graduation.  \\n\n",
    "3. The resume contains the work history of all the companies a person worked for.  \n",
    "Each job may contain the start and end dates, and job title. If the end date is \n",
    "missing then, the job is the current job.  \n",
    "4. Each job has the job titles, and associated responsibilities or experience of \n",
    "that person. Each job is independent from other jobs in the same document. \n",
    "5. The resume may contain any awards or social networking information of this person. \\n\n",
    "6. The first job should have the farest year from current date, the last or current job \n",
    "has the closest to the current date.  From the last and first job you should be able \n",
    "to calculate the total year a person has worked for each job and total years of a person\n",
    "has been worked.\\n \n",
    "\n",
    "Original question: {question}\"\"\"\n",
    "'''\n",
    "\n",
    "prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_perspectives \n",
    "    | ChatOpenAI(temperature=0) \n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi Query: Different Perspectives\n",
    "from langchain.load import dumps, loads\n",
    "\n",
    "\n",
    "def get_unique_union(documents: list[list]):\n",
    "    \"\"\" Unique union of retrieved docs \"\"\"\n",
    "    # Flatten list of lists, and convert each Document to string\n",
    "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    # Get unique documents\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    # Return\n",
    "    return [loads(doc) for doc in unique_docs]\n",
    "\n",
    "# Retrieve\n",
    "#question = \"Which Universities Sam have attended to?\"\n",
    "# Retrieve\n",
    "# with set_debug and set_verbose, you can see the intermediated questions generated from original question.\n",
    "\n",
    "from langchain.globals import set_verbose, set_debug\n",
    "\n",
    "#set_debug(True)\n",
    "#set_verbose(True)\n",
    "\n",
    "retrieval_chain = generate_queries | retriever.map() | get_unique_union\n",
    "#docs = retrieval_chain.invoke({\"question\":question})\n",
    "#len(docs)\n",
    "\n",
    "#set_debug(False)\n",
    "#set_verbose(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi Query: Different Perspectives\n",
    "from operator import itemgetter\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# RAG\n",
    "'''\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "'''\n",
    "\n",
    "template =  \"\"\"Answer the question based only on the following context \n",
    "with assumptions that A resume 0. The file name is listed in the first line,\n",
    "1. The resume contains the name, the phone number and the address of a person, \\n\n",
    "2. The resume contains the degree and attending university and years of graduation.  \\n\n",
    "3. The resume contains the work history of all the companies a person worked for.  \n",
    "Each job may contain the start and end dates, and job title. If the end date is \n",
    "missing then, the job is the current job.  \n",
    "4. Each job has the job titles, and associated responsibilities or experience of \n",
    "that person. Each job is independent from other jobs in the same document. \n",
    "5. The resume may contain any awards or social networking information of this person. \\n\n",
    "6. The first job should have the farest year from current date, the last or current job \n",
    "has the closest to the current date.  From the last and first job you should be able \n",
    "to calculate the total year a person has worked for each job and total years of a person\n",
    "has been worked.\\n\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain, \n",
    "     \"question\": itemgetter(\"question\")} \n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Without using the Chain/Pipeline but same processes\n",
    "'''\n",
    "generate_queries = (\n",
    "    prompt_perspectives \n",
    "    | ChatOpenAI(temperature=0) \n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")\n",
    "retrieval_chain = generate_queries | retriever.map() | get_unique_union\n",
    "\n",
    "and\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain, \n",
    "     \"question\": itemgetter(\"question\")} \n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "'''\n",
    "from langchain import hub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.load import dumps, loads\n",
    "from PyPDF2 import PdfReader\n",
    "#Using CharaterTextSplitter maybe have better results than RecursiveCharacterTextSplitter\n",
    "#text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=40)\n",
    "text_splitter = CharacterTextSplitter()\n",
    "#keep the default separator ('\\n\\n') is better than separator=\"\\n\"\n",
    "\n",
    "def extract_pdf_text(file_path):\n",
    "    pdf_file = PdfReader(file_path)\n",
    "    text_data = ''\n",
    "    for pg in pdf_file.pages:\n",
    "        text_data += pg.extract_text()\n",
    "    return text_data\n",
    "\n",
    "\n",
    "def get_unique_union(documents: list[list]):\n",
    "    \"\"\" Unique union of retrieved docs \"\"\"\n",
    "    # Flatten list of lists, and convert each Document to string\n",
    "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    # Get unique documents\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    # Return\n",
    "    return [loads(doc) for doc in unique_docs]\n",
    "\n",
    "\n",
    "\n",
    "def step_by_step_query(user_question, resume):\n",
    "    from langchain_core.output_parsers import CommaSeparatedListOutputParser\n",
    "    #question = \"List all the companies Sam have worked for, please also list the year he worked for those companies\"\n",
    "# Multi Query: Different Perspectives\n",
    "    question = user_question\n",
    "    pdf_text = extract_pdf_text(resume)\n",
    "    pdf_texts = text_splitter.split_text(pdf_text)\n",
    "    split_docs = text_splitter.create_documents(pdf_texts)\n",
    "    ''' Don't use OpenAIEmbeddings\n",
    "    client = OpenAI()\n",
    "    embedding_repsonse = client.embeddings.create(input=sentences, \n",
    "                                      model=\"text-embedding-3-large\")\n",
    "     '''\n",
    "    \n",
    "    vectorstore = Chroma.from_documents(documents=split_docs, \n",
    "                                    embedding=OpenAIEmbeddings(model=\"text-embedding-3-small\"))\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4}, max_tokens_limit=10000)\n",
    "    \n",
    "    template = \"\"\"You are an AI language model assistant. Your task is to generate 5 different versions of the given user question to retrieve relevant documents from a vector \n",
    "database. By generating multiple perspectives on the user question, your goal is to help the user overcome some of the limitations of the distance-based similarity search.  Provide these alternative questions separated by newlines.  Original question: {question}\"\"\"\n",
    "    prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "    prompt_perspectives_output =  prompt_perspectives.invoke(question)\n",
    "    #print(\"Output after prompt_perspectives:\", prompt_perspectives_output)\n",
    "\n",
    "    chat_openai_output = ChatOpenAI(temperature=0)(prompt_perspectives_output)\n",
    "    ##print(\"Output after ChatOpenAI:\", chat_openai_output)\n",
    "    #output_parser = CommaSeparatedListOutputParser()\n",
    "    parser_output = StrOutputParser().invoke(chat_openai_output)\n",
    "    #print(\"output after StrOutputParser:\", parser_output)\n",
    "    #str_output_parser_output = output_parser(chat_openai_output)\n",
    "    #print(\"Output after StrOutputParser:\", str_output_parser_output)\n",
    "\n",
    "    generate_queries= (lambda x: x.split(\"\\n\"))(parser_output)\n",
    "\n",
    "    print (generate_queries)\n",
    "\n",
    "    #retrieval_chain = generate_queries | retriever.map() | get_unique_union\n",
    "    # and \n",
    "    #docs = retrieval_chain.invoke({\"question\":question})\n",
    "\n",
    "    retriever_output = retriever.map().invoke(generate_queries)\n",
    "\n",
    "    #print(\"output after retriever map()\", retriever_output)\n",
    "\n",
    "    get_union_doc_output = get_unique_union(retriever_output)\n",
    "\n",
    "    #print(\"output after get_unique_union\", get_union_doc_output)\n",
    "\n",
    "    #docs = retrieval_chain.invoke({\"question\":question})\n",
    "    #docs = get_union_doc_output\n",
    "\n",
    "    #print(\"output after retriever_chain\", docs)\n",
    "\n",
    "    template =  \"\"\"Answer the question based only on the following context \n",
    "    with assumptions that A resume may include\n",
    "    0. The file name is listed in the first line,\n",
    "    1. The resume contains the name, the phone number and the address of a person, \\n\n",
    "    2. The resume contains the degree and attending university and years of graduation.  \\n\n",
    "    3. The resume contains the work history of all the companies a person worked for.  \n",
    "    Each job may contain the start and end dates, and job title. If the end date is \n",
    "    missing then, the job is the current job.  \n",
    "    4. Each job has the job titles, and associated responsibilities or experience of \n",
    "    that person. Each job is independent from other jobs in the same document. \n",
    "    5. The resume may contain any awards or social networking information of this person. \\n\n",
    "    6. The first job should have the farest year from current date, the last or current job \n",
    "    has the closest to the current date.  From the last and first job you should be able \n",
    "    to calculate the total year a person has worked for each job and total years of a person\n",
    "    has been worked.\\n\n",
    "    {context}\n",
    "\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    #prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "    llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "\n",
    "    #docsf = retriever.map().invoke({\"question\":question})\n",
    "\n",
    "    #print(\"output after retriever_chain again.. \", docsf)\n",
    "    prompt_outputf =  prompt.invoke({\"context\": get_union_doc_output[0], \"question\":question})\n",
    "    #print(\"output from prompt again\", prompt_outputf)\n",
    "    llm_output = llm(prompt_outputf)\n",
    "    #print(\"Output from llm again\", llm_output)\n",
    "    print (llm_output.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1. Can you provide a list of companies where Sam has been employed along with the corresponding years of his employment?', '2. Which companies has Sam worked for, and can you include the years during which he was employed at each company?', '3. Please list the companies that Sam has worked at, and include the years of his employment at each company.', '4. Could you give me a rundown of the companies where Sam has held positions, including the years he worked at each company?', \"5. I'm interested in knowing the companies that Sam has worked for and the specific years he was employed at each company. Can you provide this information?\"]\n",
      "1. Tarana Wireless - 10/2022 - 12/2023\n",
      "2. Miso Robotics - 11/2021 - 09/2022\n",
      "3. Quasar Science - 6/2020 - 11/2021\n",
      "4. Riverbed/Xirrus Inc - 10/2012 - 9/2019\n",
      "5. JigoCity/Ecommerce - 1/2011 - 5/2012\n",
      "6. Concordware International/China Offshore Software Development - 5/2009 - 10/2010\n",
      "7. Asoka USA - 8/2007 - 8/2008\n",
      "8. Boingo Wireless - 11/2006 - 3/2007\n",
      "9. Infospace Mobile Division - 7 - 11/2006\n",
      "10. Telemac - 8/2004 - 6/2006\n"
     ]
    }
   ],
   "source": [
    "question = \"List all the companies Sam have worked for, please also list the year he worked for those companies\"\n",
    "resume='.\\docs\\samcyangResume_Gen123.pdf'\n",
    "step_by_step_query(question, resume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"1. What is Sam's background in hardware and software development?\", '2. Can Sam demonstrate proficiency in both hardware and software development?', '3. Has Sam worked on projects involving both hardware and software development?', '4. Is Sam skilled in both hardware and software development?', \"5. Are there any examples of Sam's experience in hardware and software development?\"]\n",
      "Yes, Sam has experience in both hardware and software development.\n"
     ]
    }
   ],
   "source": [
    "question = \"Does Sam have experience in both hardware and software development\"\n",
    "resume='.\\docs\\samcyangResume_Gen123.pdf'\n",
    "step_by_step_query(question, resume)\n",
    "#using default separator in splitter is better than setting separator='\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1. What is the duration of NPI experience that Sam possesses, and which companies has he gained this experience in?', '2. In which companies has Sam accumulated his NPI experience, and for how many years?', '3. How long has Sam been involved in NPI, and which companies have contributed to his experience?', '4. Can you provide information on the number of years Sam has worked in NPI, and the companies where he has gained this experience?', '5. Which companies has Sam worked for to gain his NPI experience, and for how many years has he been involved in this field?']\n",
      "Sam has 15+ years of NPI experience and has worked in the following companies for NPI roles:\n",
      "1. Tarana Wireless from 10/2022 to 12/2023\n",
      "2. Miso Robotics from 11/2021 to 09/2022\n",
      "3. Quasar Science from 6/2020 to 11/2021\n",
      "4. Riverbed/Xirrus Inc from 10/2012 to 9/2019\n"
     ]
    }
   ],
   "source": [
    "question = \"how many years of NPI experience Sam has it and in what companies\"\n",
    "resume='.\\docs\\samcyangResume_Gen123.pdf'\n",
    "step_by_step_query(question, resume)\n",
    "#Doesn't answer the years correctly, may need to find-tune the template#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"1. What is Sam's experience with AI and robotics, and can you provide details on the last company he worked for and the years he utilized his AI and robotics skills?\", \"2. Can you tell me about Sam's background in AI and robotics, including information on the most recent company where he applied his expertise and the duration of his involvement?\", \"3. How extensive is Sam's experience in AI and robotics, and could you specify the company where he last worked with these technologies along with the timeframe?\", \"4. I'm interested in Sam's AI and robotics experience - could you share details about the last company he was employed at and the years during which he gained experience in these fields?\", \"5. What can you tell me about Sam's involvement in AI and robotics, particularly at his most recent company? Please include the years he spent utilizing his skills in these areas.\"]\n",
      "Yes, Sam has AI and robotics experience. The last company where he uses AI and Robotics experience is Miso Robotics from 11/2021 - 09/2022.\n"
     ]
    }
   ],
   "source": [
    "# llm responses the wrong answer if you ask list all companies he uses AI and robotics,\n",
    "question = \"Does Sam have AI and robotics experience, and please list the last company and years he uses the AI and Robotics experience?\"\n",
    "resume='.\\docs\\samcyangResume_Gen123.pdf'\n",
    "step_by_step_query(question, resume)\n",
    "#The answer is not ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cpu",
   "language": "python",
   "name": "cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
