{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "RAG - Multi-Query Query translation\n",
    "Besides using the LangChain Chain or Pipeline illustrating in the LangChain Rag tutorial, \n",
    "the function in the last cell executes the same logic, step by step, which helps me to \n",
    "understand the processes and also be able to improve the prompt better without \n",
    "switching to langsmith often.\n",
    "\n",
    "Please review the RAG tutorial from Langchain in details (part 5 - multi-query query translation)\n",
    "https://github.com/langchain-ai/rag-from-scratch/blob/main/rag_from_scratch_1_to_4.ipynb\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "#RAG-- Common RAG \n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "#if (GCP_PROJECT_ID == None): print (\"Not set\")\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "LANGCHAIN_API_KEY = os.getenv('LANGCHAIN_API_KEY')\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'trace'  #true for trace\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"RAG - Multi-Query-Translation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/langchain-ai/rag-from-scratch/blob/main/rag_from_scratch_1_to_4.ipynb\n",
    "#RAG-- Common RAG \n",
    "# import bs4\n",
    "from langchain import hub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "global_index = 0\n",
    "#Using CharaterTextSplitter is much better than RecursiveCharacterTextSplitter\n",
    "#text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=40)\n",
    "text_splitter = CharacterTextSplitter()\n",
    "#Chunk size has no effect on CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Example: Document retriving from Web ####\n",
    "'''\n",
    "# Load Documents from Web\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "doc = loader.load()\n",
    "docs = text_splitter.split_documents(doc)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Indexing\n",
    "#Loading Document from PDF, Sam's resume from local,  instead of Web\n",
    "#RAG-- Common Rag\n",
    "\n",
    "from PyPDF2 import PdfReader\n",
    "def extract_pdf_text(file_path):\n",
    "    pdf_file = PdfReader(file_path)\n",
    "    text_data = ''\n",
    "    for pg in pdf_file.pages:\n",
    "        text_data += pg.extract_text()\n",
    "    return text_data\n",
    "\n",
    "pdf_text = extract_pdf_text('c:\\\\workspace\\\\python\\\\csv\\\\docs\\\\samcyangResume_Gen123.pdf')\n",
    "pdf_texts = text_splitter.split_text(pdf_text)\n",
    "split_docs = text_splitter.create_documents(pdf_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieving\n",
    "# RAG-- Common Rag\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=split_docs, \n",
    "                                    embedding=OpenAIEmbeddings(model=\"text-embedding-3-small\"))\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4}, max_tokens_limit=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, Multi-Query or Query-Translation\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "question = \"List all the companies Sam have worked for, please also list the year he worked for those companies\"\n",
    "# Multi Query: Different Perspectives\n",
    "\n",
    "template = \"\"\"You are an AI language model assistant. Your task is to generate 5\n",
    "different versions of the given user question to retrieve relevant documents from a vector \n",
    "database. By generating multiple perspectives on the user question, your goal is to help\n",
    "the user overcome some of the limitations of the distance-based similarity search. \n",
    "Provide these alternative questions separated by newlines.  Original question: {question}\"\"\"\n",
    "'''\n",
    "template = \"\"\"A resume contains 1. the name, the phone number and \n",
    "the address of a person, 2.  the education or attending Schools and years \n",
    "of graduation.  3. the companies a person worked for with the job titles, \n",
    "starting and ending dates, and responsibilities of that person.  4. the \n",
    "awards or social networking information of this person. Original question: {question}\"\"\"\n",
    "'''\n",
    "\n",
    "prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_perspectives \n",
    "    | ChatOpenAI(temperature=0) \n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi Query: Different Perspectives\n",
    "from langchain.load import dumps, loads\n",
    "\n",
    "\n",
    "def get_unique_union(documents: list[list]):\n",
    "    \"\"\" Unique union of retrieved docs \"\"\"\n",
    "    # Flatten list of lists, and convert each Document to string\n",
    "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    # Get unique documents\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    # Return\n",
    "    return [loads(doc) for doc in unique_docs]\n",
    "\n",
    "# Retrieve\n",
    "#question = \"Which Universities Sam have attended to?\"\n",
    "# Retrieve\n",
    "# with set_debug and set_verbose, you can see the intermediated questions generated from original question.\n",
    "\n",
    "from langchain.globals import set_verbose, set_debug\n",
    "\n",
    "#set_debug(True)\n",
    "#set_verbose(True)\n",
    "\n",
    "retrieval_chain = generate_queries | retriever.map() | get_unique_union\n",
    "docs = retrieval_chain.invoke({\"question\":question})\n",
    "len(docs)\n",
    "\n",
    "#set_debug(False)\n",
    "#set_verbose(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi Query: Different Perspectives\n",
    "from operator import itemgetter\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# RAG\n",
    "'''\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "'''\n",
    "\n",
    "template =  \"\"\"Answer the question based only on the following context \n",
    "with assumptions that A resume contains 1. the name, the phone number and \n",
    "the address of a person, 2.  the education or attending Schools and years \n",
    "of graduation.  3. the companies a person worked for with the job titles, \n",
    "starting and ending dates, and responsibilities of that person.  4. the \n",
    "awards or social networking information of this person.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain, \n",
    "     \"question\": itemgetter(\"question\")} \n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instead of using Pipeline/Chain but step by step that can helpt od understand\n",
    "# the process and enanable for easy debugging\n",
    "'''\n",
    "generate_queries = (\n",
    "    prompt_perspectives \n",
    "    | ChatOpenAI(temperature=0) \n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")\n",
    "retrieval_chain = generate_queries | retriever.map() | get_unique_union\n",
    "\n",
    "and\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain, \n",
    "     \"question\": itemgetter(\"question\")} \n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "'''\n",
    "from langchain import hub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.load import dumps, loads\n",
    "from PyPDF2 import PdfReader\n",
    "#Using CharaterTextSplitter is much better than RecursiveCharacterTextSplitter\n",
    "#text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=40)\n",
    "text_splitter = CharacterTextSplitter()\n",
    "\n",
    "def extract_pdf_text(file_path):\n",
    "    pdf_file = PdfReader(file_path)\n",
    "    text_data = ''\n",
    "    for pg in pdf_file.pages:\n",
    "        text_data += pg.extract_text()\n",
    "    return text_data\n",
    "\n",
    "\n",
    "def get_unique_union(documents: list[list]):\n",
    "    \"\"\" Unique union of retrieved docs \"\"\"\n",
    "    # Flatten list of lists, and convert each Document to string\n",
    "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    # Get unique documents\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    # Return\n",
    "    return [loads(doc) for doc in unique_docs]\n",
    "\n",
    "\n",
    "\n",
    "def step_by_step_query(user_question, resume):\n",
    "    from langchain_core.output_parsers import CommaSeparatedListOutputParser\n",
    "    #question = \"List all the companies Sam have worked for, please also list the year he worked for those companies\"\n",
    "# Multi Query: Different Perspectives\n",
    "    question = user_question\n",
    "    pdf_text = extract_pdf_text(resume)\n",
    "    pdf_texts = text_splitter.split_text(pdf_text)\n",
    "    split_docs = text_splitter.create_documents(pdf_texts)\n",
    "    \n",
    "    vectorstore = Chroma.from_documents(documents=split_docs, \n",
    "                                    embedding=OpenAIEmbeddings(model=\"text-embedding-3-small\"))\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4}, max_tokens_limit=10000)\n",
    "    \n",
    "    template = \"\"\"You are an AI language model assistant. Your task is to generate 5 different versions of the given user question to retrieve relevant documents from a vector \n",
    "database. By generating multiple perspectives on the user question, your goal is to help the user overcome some of the limitations of the distance-based similarity search.  Provide these alternative questions separated by newlines.  Original question: {question}\"\"\"\n",
    "\n",
    "    prompt_perspectives_output =  prompt_perspectives.invoke(question)\n",
    "    #print(\"Output after prompt_perspectives:\", prompt_perspectives_output)\n",
    "\n",
    "    chat_openai_output = ChatOpenAI(temperature=0)(prompt_perspectives_output)\n",
    "    ##print(\"Output after ChatOpenAI:\", chat_openai_output)\n",
    "    #output_parser = CommaSeparatedListOutputParser()\n",
    "    parser_output = StrOutputParser().invoke(chat_openai_output)\n",
    "    #print(\"output after StrOutputParser:\", parser_output)\n",
    "    #str_output_parser_output = output_parser(chat_openai_output)\n",
    "    #print(\"Output after StrOutputParser:\", str_output_parser_output)\n",
    "\n",
    "    final_output = (lambda x: x.split(\"\\n\"))(parser_output)\n",
    "    #print(\"Final Output:\", final_output)\n",
    "\n",
    "    generate_queries = final_output\n",
    "    # now let's manually do the \n",
    "    #retrieval_chain = generate_queries | retriever.map() | get_unique_union\n",
    "    # and \n",
    "    #docs = retrieval_chain.invoke({\"question\":question})\n",
    "\n",
    "\n",
    "    retriever_output = retriever.map().invoke(final_output)\n",
    "\n",
    "    #print(\"output after retriever map()\", retriever_output)\n",
    "\n",
    "    get_union_doc_output = get_unique_union(retriever_output)\n",
    "\n",
    "    #print(\"output after get_unique_union\", get_union_doc_output)\n",
    "\n",
    "    docs = retrieval_chain.invoke({\"question\":question})\n",
    "\n",
    "    #print(\"output after retriever_chain\", docs)\n",
    "\n",
    "    template =  \"\"\"Answer the question based only on the following context with assumptions that A resume contains 1. the name, the phone number and the address of a person, 2.  the education or attending Schools and years \n",
    "of graduation.  3. the companies a person worked for with the job titles, starting and ending dates, and responsibilities of that person.  4. the awards or social networking information of this person.\n",
    "\n",
    "    {context}\n",
    "\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    #prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "    llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "\n",
    "    docsf = retrieval_chain.invoke({\"question\":question})\n",
    "\n",
    "    #print(\"output after retriever_chain again.. \", docsf)\n",
    "    prompt_outputf =  prompt.invoke({\"context\": docsf, \"question\":question})\n",
    "    #print(\"output from prompt again\", prompt_outputf)\n",
    "    llm_output = llm(prompt_outputf)\n",
    "    #print(\"Output from llm again\", llm_output)\n",
    "    print (llm_output.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Tarana Wireless - 10/2022 - 12/2023\n",
      "2. Miso Robotics - 11/2021 - 09/2022\n",
      "3. Quasar Science - 6/2020 - 11/2021\n",
      "4. Riverbed/Xirrus Inc - 10/2012 - 9/2019\n",
      "5. JigoCity/Ecommerce - 1/2011 - 5/2012\n",
      "6. Concordware International/China Offshore Software Development - 5/2009 - 10/2010\n",
      "7. Asoka USA - 8/2007 - 8/2008\n",
      "8. Boingo Wireless - 11/2006 - 3/2007\n",
      "9. Infospace Mobile Division - 7-11/2006\n",
      "10. Telemac - 8/2004 - 6/2006\n"
     ]
    }
   ],
   "source": [
    "question = \"List all the companies Sam have worked for, please also list the year he worked for those companies\"\n",
    "resume='.\\samcyangResume_Gen123.pdf'\n",
    "step_by_step_query(question, resume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, Sam C. Yang has experience in both hardware and software development. This can be inferred from his roles such as NPI Program Manager at Quasar Science, where he worked with internal and external engineer teams on hardware design, and as a Director of Engineering at Boingo Wireless, where he implemented, enhanced, operated, and maintained WISP authentication systems.\n"
     ]
    }
   ],
   "source": [
    "question = \"Does Sam have experience in both hardware and software development\"\n",
    "resume='.\\samcyangResume_Gen123.pdf'\n",
    "step_by_step_query(question, resume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sam has 15+ years of NPI experience. He gained this experience at the following companies:\n",
      "1. Tarana Wireless\n",
      "2. Quasar Science\n",
      "3. Riverbed/Xirrus Inc\n"
     ]
    }
   ],
   "source": [
    "question = \"how many years of NPI experience Sam has it and in what companies\"\n",
    "resume='.\\samcyangResume_Gen456.pdf'\n",
    "step_by_step_query(question, resume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, Sam has AI and robotics experience. The last company where he used his AI and Robotics experience is Miso Robotics from 11/2021 - 09/2022.\n"
     ]
    }
   ],
   "source": [
    "# llm responses the wrong answer if you ask list all companies he uses AI and robotics,\n",
    "question = \"Does Sam have AI and robotics experience, and please list the last company and years he uses the AI and Robotics experience?\"\n",
    "resume='.\\samcyangResume_Gen456.pdf'\n",
    "step_by_step_query(question, resume)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cpu",
   "language": "python",
   "name": "cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
