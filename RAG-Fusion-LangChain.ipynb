{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "RAG - Fusion Query translation\n",
    "\n",
    "Besides using the LangChain Chain/Pipeline as illustrated in the LangChain Rag tutorial \n",
    "below, this program reads the PDF resumes from a directory, each resume will be saved \n",
    "as a document without duplication. Â \n",
    "\n",
    "We also create a single step_by_step_fusion (question, template) program \n",
    "that executes the same logic but without the abstract of Chain/Pipeline. \n",
    "It helps me to understand the processes and also be able to improve the template \n",
    "(prompt or question) preparation better. Of course, langSmith is a great tool as well.\n",
    "\n",
    "The template input that passes to step_by_step_fusion is the format you \n",
    "want the ChatPGT to return such that the final process of Q&A should have a better answer.\n",
    "\n",
    "Please review the RAG tutorial from Langchain in details \n",
    "(part 6 - multi-fusion query translation)https://www.youtube.com/watch?v=77qELPbNgxA\n",
    "'''\n",
    "\n",
    "#RAG-- Common Rag\n",
    "# Query Translation:   Multi-Query, Fusion and Decompsoition \n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "#if (GCP_PROJECT_ID == None): print (\"Not set\")\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "LANGCHAIN_API_KEY = os.getenv('LANGCHAIN_API_KEY')\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'false'  #true for trace\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#RAG-- Common Rag\n",
    "# Query Translation:   Multi-Query, Fusion and Decompsoition \n",
    "from langchain import hub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"RAG Fusion\"\n",
    "\n",
    "#text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=40)\n",
    "text_splitter = CharacterTextSplitter()\n",
    "question = \"Please identify all the documents that have the robotic experience\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RAG-- Common Rag\n",
    "# Query Translation:   Multi-Query, Fusion and Decompsoition \n",
    "'''\n",
    "from PyPDF2 import PdfReader\n",
    "def extract_pdf_text(file_path):\n",
    "    pdf_file = PdfReader(file_path)\n",
    "    text_data = ''\n",
    "    for pg in pdf_file.pages:\n",
    "        text_data += pg.extract_text()\n",
    "    return text_data\n",
    "\n",
    "pdf_text = extract_pdf_text('c:\\\\workspace\\\\python\\\\csv\\\\docs\\\\samcyangResume_Gen123.pdf')\n",
    "pdf_texts = text_splitter.split_text(pdf_text)\n",
    "split_docs = text_splitter.create_documents(pdf_texts)\n",
    "'''\n",
    "\n",
    "import uuid\n",
    "import hashlib\n",
    "from PyPDF2 import PdfReader\n",
    "def extract_pdf_text(file_path):\n",
    "    pdf_file = PdfReader(file_path)\n",
    "    text_data = ''\n",
    "    for pg in pdf_file.pages:\n",
    "        text_data += pg.extract_text()\n",
    "    return text_data\n",
    "\n",
    "\n",
    "resume_dir = \".\\\\docs\\\\\"\n",
    "pdf_text = []\n",
    "\n",
    "\n",
    "#Change 1\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = Chroma(\"langchain\", embeddings)\n",
    "for file in os.listdir(resume_dir):\n",
    "    filepath = os.path.join(resume_dir,file)\n",
    "    collection = vectorstore.get('langchain')\n",
    "    if (filepath.endswith('.pdf')): \n",
    "            #Change 2\n",
    "        pdf_text=[]\n",
    "        pdf_text.append (\"File Name: \"+filepath+\"  \\n\"+extract_pdf_text(filepath))\n",
    "        split_docs = text_splitter.create_documents(pdf_text)\n",
    "        existing = vectorstore.get(file)\n",
    "        \n",
    "        if (existing['ids'] != [] and existing['ids'][0] == file):\n",
    "            #print(\"Deleting Duplication .....\", file)\n",
    "            vectorstore.delete(file)\n",
    "            # Add documents back to collection\n",
    "        try:\n",
    "            #print(\"Split Doc   \", split_docs)\n",
    "                #Need to provide IDS list to add_documents, otherwise, it will only pick up the first character of the file name\n",
    "            langchain_ids = vectorstore.add_documents(ids=[file], documents=split_docs) \n",
    "                #print(\"Adding Langchain ID - \", langchain_ids, \" File Name - \", file)\n",
    "                #langchain_ids should be equal to file str\n",
    "        except:\n",
    "                #print(\"Again....Deleting Duplication .....\", file)\n",
    "            vectorstore.delete(file)\n",
    "                #print(\"Existing... \", existing['ids'])\n",
    "            print(\"Can't add.. \", file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieving\n",
    "# RAG-- Common Rag\n",
    "#Change 1\n",
    "'''vectorstore = Chroma.from_documents(documents=split_docs, \n",
    "                                    embedding=OpenAIEmbeddings(model=\"text-embedding-3-small\"))\n",
    "'''\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4}, max_tokens_limit=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Retrieving the relevant embedding documents \n",
    "# \n",
    "#docs = retriever.get_relevant_documents(question)\n",
    "#len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RAG-Fusion\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"You are a helpful assistant that generates multiple sub-questions related to an input question about a resume. \\n\n",
    "The goal is generate multiple search queries related to the experience listed in a resume, and each sub-question that can be answers in separately \\n\n",
    "However, you must keep the main point in the original questions \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (3 queries):\"\"\"\n",
    "# RAG-Fusion: Related\n",
    "'''\n",
    "template = \"\"\"You are a helpful assistant that generates multiple search queries based on a single input query. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (4 queries):\"\"\"\n",
    "'''\n",
    "prompt_rag_fusion = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RAG-Fusion\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "generate_queries = (\n",
    "    prompt_rag_fusion \n",
    "    | llm\n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RAG-Fusion\n",
    "from langchain.load import dumps, loads\n",
    "\n",
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    \"\"\" Reciprocal_rank_fusion that takes multiple lists of ranked documents \n",
    "        and an optional parameter k used in the RRF formula \"\"\"\n",
    "    \n",
    "    # Initialize a dictionary to hold fused scores for each unique document\n",
    "    fused_scores = {}\n",
    "    print (\"results....\", results)\n",
    "    # Iterate through each list of ranked documents\n",
    "    for docs in results:\n",
    "        print (\"docs... \", docs)\n",
    "        # Iterate through each document in the list, with its rank (position in the list)\n",
    "        for rank, doc in enumerate(docs):\n",
    "            print (doc)\n",
    "            # Convert the document to a string format to use as a key (assumes documents can be serialized to JSON)\n",
    "            doc_str = dumps(doc)\n",
    "            print(\"rank...\", rank, \"doc..\", doc_str)\n",
    "            # If the document is not yet in the fused_scores dictionary, add it with an initial score of 0\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            # Retrieve the current score of the document, if any\n",
    "            previous_score = fused_scores[doc_str]\n",
    "            # Update the score of the document using the RRF formula: 1 / (rank + k)\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "        print (\"Fused Scores::\", len(fused_scores))\n",
    "        for i in fused_scores: print(\"....\", fused_scores[i])\n",
    "    # Sort the documents based on their fused scores in descending order to get the final reranked results\n",
    "    reranked_results = [\n",
    "        (loads(doc), score)\n",
    "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "    print(reranked_results)\n",
    "    # Return the reranked results as a list of tuples, each containing the document and its fused score\n",
    "    return reranked_results\n",
    "\n",
    "retrieval_chain_rag_fusion = generate_queries | retriever.map() | reciprocal_rank_fusion\n",
    "docs = retrieval_chain_rag_fusion.invoke({\"question\": question})\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#docs[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RAG-Fusion\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from operator import itemgetter\n",
    "# RAG\n",
    "template = \"\"\"Answer the following question based on these context \\n\n",
    "and return two documents that can best match the question. Please note, the first line\n",
    "of each document contains the filename, you only need to return the file names.:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain_rag_fusion, \n",
    "     \"question\": itemgetter(\"question\")} \n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step_by_step_fusion())\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain import hub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "import uuid\n",
    "import hashlib\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.load import dumps, loads\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from operator import itemgetter\n",
    "question = \"Please identify all the documents that have the robotic experience\"\n",
    "\n",
    "def step_by_step_fusion(question_input, template_input):\n",
    "    question = question_input\n",
    "    load_dotenv()\n",
    "    #if (GCP_PROJECT_ID == None): print (\"Not set\")\n",
    "    OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "    LANGCHAIN_API_KEY = os.getenv('LANGCHAIN_API_KEY')\n",
    "\n",
    "\n",
    "    os.environ['LANGCHAIN_TRACING_V2'] = 'false'  #true for trace\n",
    "    os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "    os.environ[\"LANGCHAIN_PROJECT\"] = \"RAG Fusion\"\n",
    "\n",
    "    def create_uuid_from_string(val: str):\n",
    "        hex_string = hashlib.md5(val.encode(\"UTF-8\")).hexdigest()\n",
    "        return uuid.UUID(hex=hex_string)\n",
    "\n",
    "    def extract_pdf_text(file_path):\n",
    "        pdf_file = PdfReader(file_path)\n",
    "        text_data = ''\n",
    "        for pg in pdf_file.pages:\n",
    "            text_data += pg.extract_text()\n",
    "        return text_data\n",
    "\n",
    "    def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "        \"\"\" Reciprocal_rank_fusion that takes multiple lists of ranked documents \n",
    "            and an optional parameter k used in the RRF formula \"\"\"\n",
    "    \n",
    "        # Initialize a dictionary to hold fused scores for each unique document\n",
    "        fused_scores = {}\n",
    "        #print (\"results....\", results)\n",
    "        # Iterate through each list of ranked documents\n",
    "        for docs in results:\n",
    "            #print (\"docs... \", docs)\n",
    "            # Iterate through each document in the list, with its rank (position in the list)\n",
    "            for rank, doc in enumerate(docs):\n",
    "            # Convert the document to a string format to use as a key (assumes documents can be serialized to JSON)\n",
    "                doc_str = dumps(doc)\n",
    "                #print(\"rank...\", rank, \"doc..\", doc_str)\n",
    "                # If the document is not yet in the fused_scores dictionary, add it with an initial score of 0\n",
    "                if doc_str not in fused_scores:\n",
    "                    fused_scores[doc_str] = 0\n",
    "                # Retrieve the current score of the document, if any\n",
    "                previous_score = fused_scores[doc_str]\n",
    "                # Update the score of the document using the RRF formula: 1 / (rank + k)\n",
    "                fused_scores[doc_str] += 1 / (rank + k)\n",
    "            #print (\"Fused Scores::\", len(fused_scores))\n",
    "            #for i in fused_scores: \n",
    "                #print(\"....\", fused_scores[i])\n",
    "        # Sort the documents based on their fused scores in descending order to get the final reranked results\n",
    "        reranked_results = [\n",
    "            (loads(doc), score)\n",
    "            for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        ]\n",
    "        #print(reranked_results)\n",
    "        # Return the reranked results as a list of tuples, each containing the document and its fused score\n",
    "        return reranked_results\n",
    "\n",
    "\n",
    "    #---------------------------Start\n",
    "    resume_dir = \".\\\\docs\\\\\"\n",
    "    pdf_text = []\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    vectorstore = Chroma(\"langchain\", embeddings)\n",
    "    llm = ChatOpenAI(temperature=0)\n",
    "    #text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=40)\n",
    "    text_splitter = CharacterTextSplitter()\n",
    "\n",
    "    for file in os.listdir(resume_dir):\n",
    "        filepath = os.path.join(resume_dir,file)\n",
    "        collection = vectorstore.get('langchain')\n",
    "        if (filepath.endswith('.pdf')): \n",
    "            #Change 2\n",
    "            pdf_text=[]\n",
    "            pdf_text.append (\"File Name: \"+filepath+\"  \\n\"+extract_pdf_text(filepath))\n",
    "            split_docs = text_splitter.create_documents(pdf_text)\n",
    "            existing = vectorstore.get(file)\n",
    "        \n",
    "            if (existing['ids'] != [] and existing['ids'][0] == file):\n",
    "                #print(\"Deleting Duplication .....\", file)\n",
    "                vectorstore.delete(file)\n",
    "            # Add documents back to collection\n",
    "            try:\n",
    "                #print(\"Split Doc   \", split_docs)\n",
    "                #Need to provide IDS list to add_documents, otherwise, it will only pick up the first character of the file name\n",
    "                langchain_ids = vectorstore.add_documents(ids=[file], documents=split_docs) \n",
    "                #print(\"Adding Langchain ID - \", langchain_ids, \" File Name - \", file)\n",
    "                #langchain_ids should be equal to file str\n",
    "            except:\n",
    "                #print(\"Again....Deleting Duplication .....\", file)\n",
    "                vectorstore.delete(file)\n",
    "                #print(\"Existing... \", existing['ids'])\n",
    "                print(\"Can't add.. \", file)\n",
    "\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2}, max_tokens_limit=10000)           \n",
    "    #docs = retriever.get_relevant_documents(question)\n",
    "    #len(docs)  \n",
    "    \n",
    "\n",
    "    template = \"\"\"You are a helpful assistant that generates multiple sub-questions related to an input question about a resume. \\n\n",
    "The goal is generate multiple search queries related to the experience listed in a resume, and each sub-question that can be answers in separately \\n\n",
    "However, you must keep the main point in the original questions \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (3 queries):\"\"\"\n",
    "\n",
    "    prompt_rag_fusion = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "    prompt_perspectives_output =  prompt_rag_fusion.invoke(question)\n",
    "        #print(\"Output after prompt_perspectives:\", prompt_perspectives_output)\n",
    "\n",
    "    chat_openai_output = ChatOpenAI(temperature=0)(prompt_perspectives_output)\n",
    "        ##print(\"Output after ChatOpenAI:\", chat_openai_output)\n",
    "        #output_parser = CommaSeparatedListOutputParser()\n",
    "    parser_output = StrOutputParser().invoke(chat_openai_output)\n",
    "        #print(\"output after StrOutputParser:\", parser_output)\n",
    "        #str_output_parser_output = output_parser(chat_openai_output)\n",
    "        #print(\"Output after StrOutputParser:\", str_output_parser_output)\n",
    "\n",
    "    final_output = (lambda x: x.split(\"\\n\"))(parser_output)\n",
    "        #print(\"Final Output:\", final_output)\n",
    "\n",
    "\n",
    "\n",
    "    retriever_output = retriever.map().invoke(final_output)\n",
    "\n",
    "    reciprocal_output = reciprocal_rank_fusion(retriever_output)\n",
    "\n",
    "    template = template_input + \"\"\"\n",
    "\n",
    "    {context}\n",
    "\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "    prompt_output = prompt.invoke({\"context\": reciprocal_output, \"question\":question})\n",
    "    #print(\"\\n\\n\")\n",
    "    #print(\"prompt_output...\", prompt_output)\n",
    "    llm_output = llm(prompt_output)\n",
    "    #print(\"Output from llm again\", llm_output)\n",
    "    print (llm_output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Please identify all the documents that have the robotic experience\"\n",
    "template =  \"\"\"Answer the following question based on these context \\n\n",
    "    and return two documents that can best match the question. Please note, the first line\n",
    "    of each document contains the filename, you only need to return the file names.:\n",
    "    \"\"\"\n",
    "step_by_step_fusion(question, template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Please list all the companies that Sam Yang has worked for\"\n",
    "template =  \"\"\"Answer the following question based on these context \\n\"\"\"\n",
    "step_by_step_fusion(question, template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Please list the name who has both Hardware and Software development experience\"\n",
    "template =  \"\"\"Answer the following question based on these context and provide the name, email and phone number\\n\"\"\"\n",
    "step_by_step_fusion(question, template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleanup the vectordb\n",
    "def clean_vectorstore():\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    ectorstore = Chroma(\"langchain\", embeddings)\n",
    "    ids = vectorstore.get().get('ids')\n",
    "    len(ids)\n",
    "    print(\"ids \", ids)\n",
    "    for id in ids: \n",
    "        docs = vectorstore.get(id)\n",
    "        print(\"docs \", docs)\n",
    "        print(id)\n",
    "        vectorstore.delete(id)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cpu",
   "language": "python",
   "name": "cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
