{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "RAG - Decomposition question to sub-questions and aggregating each sub-question/answar pair\n",
    "as part of prompt context in the next sub-question prompt till final.\n",
    "\n",
    "RAG - Decomposition question to sub-questions, and provide the final answer from the \n",
    "retrieved documents of each sub-question.\n",
    "\n",
    "Decomposition translation is different from multi-query or fusion, instead of \n",
    "creating similar questions, it creates sub-questions from the original question. \n",
    "\n",
    "step_to_step_decomposition, at end, is based on the LangChain Rag youtube tutorial below, \n",
    "the code from the tutorial is listed in the first half of the ipython notebook. \n",
    "\n",
    "This function follows the same logic from the tutorial but without the abstract of \n",
    "Chain/Pipeline. This approach helps me to understand the sub-questions creation, \n",
    "llm processes and also can improve the final prompt preparation. \n",
    "\n",
    "Of course, the langsmith is a great tool as well. \n",
    "\n",
    "I may be wrong, but it seems to me the separate the sub-questions as individual questions\n",
    "can return more accurate results than the aggregation approach. \n",
    "However, the aggregation decomposition approach provide more details in answer.\n",
    "\n",
    "\n",
    "Input arguments to the step_to_step_fusion are\n",
    "1. Quesition: question string\n",
    "2. Additional_background:  the helping instructions you want the ChatPGT to return the Q&A \n",
    "with a better answer.\n",
    "3. Aggregation: 1 means aggregating sub-questions as prompt for the next sub-question Q&A\n",
    "\n",
    "\n",
    "Please review the RAG tutorial from Langchain in details (part 7 \n",
    "- decompistion query translation)\n",
    "https://www.youtube.com/watch?v=h0OPWlEOank\n",
    "\n",
    "'''\n",
    "#RAG-- Common Rag\n",
    "#Query Translation:   Multi-Query, Fusion and Decompsoition \n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "#if (GCP_PROJECT_ID == None): print (\"Not set\")\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "LANGCHAIN_API_KEY = os.getenv('LANGCHAIN_API_KEY')\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'false'  #true for trace\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"RAG - Decomposition-Query-Translation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RAG-- Common Rag\n",
    "#Query Translation:   Multi-Query, Fusion and Decompsoition \n",
    "from langchain import hub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "#text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "text_splitter = CharacterTextSplitter()\n",
    "question = \"Please list all the documents that contains the robotic experience\"\n",
    "question=\"List all candidates by name that who have Java work experience?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RAG-- Common Rag\n",
    "#Query Translation:   Multi-Query, Fusion and Decompsoition \n",
    "#Loading PDF resumes from local instead of the Web\n",
    "\n",
    "import uuid\n",
    "import hashlib\n",
    "from PyPDF2 import PdfReader\n",
    "def extract_pdf_text(file_path):\n",
    "    pdf_file = PdfReader(file_path)\n",
    "    text_data = ''\n",
    "    for pg in pdf_file.pages:\n",
    "        text_data += pg.extract_text()\n",
    "    return text_data\n",
    "\n",
    "\n",
    "resume_dir = \".\\\\docs\\\\\"\n",
    "pdf_text = []\n",
    "\n",
    "\n",
    "#Change 1\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = Chroma(\"langchain\", embeddings)\n",
    "\n",
    "\n",
    "\n",
    "def create_uuid_from_string(val: str):\n",
    "    hex_string = hashlib.md5(val.encode(\"UTF-8\")).hexdigest()\n",
    "    return uuid.UUID(hex=hex_string)\n",
    "\n",
    "\n",
    "for file in os.listdir(resume_dir):\n",
    "    filepath = os.path.join(resume_dir,file)\n",
    "    collection = vectorstore.get('langchain')\n",
    "    if (filepath.endswith('.pdf')): \n",
    "        #Change 2\n",
    "        pdf_text=[]\n",
    "        pdf_text.append (\"File Name: \"+filepath+\"  \\n\"+extract_pdf_text(filepath))\n",
    "        split_docs = text_splitter.create_documents(pdf_text)\n",
    "        existing = vectorstore.get(file)\n",
    "       \n",
    "        if (existing['ids'] != [] and existing['ids'][0] == file):\n",
    "            print(\"Deleting Duplication .....\", file)\n",
    "            vectorstore.delete(file)\n",
    "        # Add documents back to collection\n",
    "        try:\n",
    "            #print(\"Split Doc   \", split_docs)\n",
    "            #Need to provide IDS list to add_documents, otherwise, it will only pick up the first character of the file name\n",
    "            langchain_ids = vectorstore.add_documents(ids=[file], documents=split_docs) \n",
    "            print(\"Adding Langchain ID - \", langchain_ids, \" File Name - \", file)\n",
    "            #langchain_ids should be equal to file str\n",
    "        except:\n",
    "            #print(\"Again....Deleting Duplication .....\", file)\n",
    "            vectorstore.delete(file)\n",
    "            #print(\"Existing... \", existing['ids'])\n",
    "            print(\"Can't add.. \", file)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieving\n",
    "#RAG-- Common Rag\n",
    "\n",
    "#vectorstore = Chroma.from_documents(documents=split_docs, embedding = embedding)\n",
    "#retriever=vectorstore.as_retriever(search_kwargs={\"k\": 2}, max_tokens_limit=10000)\n",
    "retriever=vectorstore.as_retriever(search_kwargs={\"k\": 2}, max_tokens_limit=10000,\n",
    "                                   collection_metadata={\"hnsw:M\": 1024,\"hnsw:ef\": 64})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decomposition - Method 1, Aggreating subquestions.\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "#question = \"Please list all the documents that contains the robotic experience\"\n",
    "#Decomposition\n",
    "template = \"\"\"You are a helpful assistant that generates multiple sub-questions related to an input question about a resume. \\n\n",
    "The goal is to break down the input question into a set of sub-question about the experience listed in a resume, and each sub-question that can be answers in separately \\n\n",
    "However, you must keep the principle of the origin question in sub-question \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (3 queries):\"\"\"\n",
    "prompt_decomposition = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decomposition - Method 1  Continue\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "#question=\"How long has each candidate been involved with robotics, and what roles have they held in this field?\"\n",
    "#question =\"Please list all the candidate name that who has the robotic experience\"\n",
    "# LLM\n",
    "question=\"List all candidates by name that who have Java work experience?\"\n",
    "llm = ChatOpenAI(temperature=0.2)\n",
    "# ***relaxing temperature between 0 - 0.5 may have a better sub-questions generated.\n",
    "# Chain\n",
    "generate_queries_decomposition = ( prompt_decomposition | llm | StrOutputParser() | (lambda x: x.split(\"\\n\")))\n",
    "# Run\n",
    "questions = generate_queries_decomposition.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decomposition - Method 1 Continue\n",
    "\n",
    "context = \"\"\"Each document (or resume) contains\n",
    "\n",
    "0. The file name at the first line,\n",
    "1. the name, the phone number and the address of a person, \\n\n",
    "2. the education or attending Schools and years of graduation.  \\n\n",
    "3. the work histroy lists all the companies a person worked for.  Each job contains the start and end dates.\n",
    "the end date can be missing since it is current job.  Each job has the job titles, and associated \n",
    "responsibilities or experience of that person. Ecah job is indpendent from other jobs in a document. \n",
    "4. Any awards or social networking information of this person. \\n\n",
    "5. Each job The first job should have the farest year from current date, the last or currnet job has the \n",
    "year closest to the current date.  From the last and first job you should be able to calcuate\n",
    "the total year a person has worked for that job \\n\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "template = \"\"\"Here is the question you need to answer:\n",
    "\n",
    "\\n --- \\n {question} \\n --- \\n\n",
    "\n",
    "Here is any available background question + answer pairs:\n",
    "\n",
    "\\n --- \\n {q_a_pairs} \\n --- \\n\n",
    "\n",
    "Here is additional context relevant to the answer of the question: \n",
    "\n",
    "\\n --- \\n {context} \\n --- \\n\n",
    "\n",
    "Use the above context and any background question + answer pairs to answer the question: \\n {question}\n",
    "\"\"\"\n",
    "\n",
    "decomposition_prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decomposition - Method 1 Continue\n",
    "from operator import itemgetter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def format_qa_pair(question, answer):\n",
    "    \"\"\"Format Q and A pair\"\"\"\n",
    "    formatted_string = \"\"\n",
    "    formatted_string += f\"Question: {question}\\nAnswer: {answer}\\n\\n\"\n",
    "    #print(\"format String...\", formatted_string.strip(), \"\\n\")\n",
    "    return formatted_string.strip()\n",
    "\n",
    "# llm\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "q_a_pairs = \"\"\n",
    "\n",
    "for q in questions:\n",
    "    rag_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | retriever, \n",
    "     \"question\": itemgetter(\"question\"),\n",
    "     \"q_a_pairs\": itemgetter(\"q_a_pairs\")} \n",
    "    | decomposition_prompt\n",
    "    | llm\n",
    "    | StrOutputParser())\n",
    "\n",
    "    answer = rag_chain.invoke({\"question\":q,\"q_a_pairs\":q_a_pairs})\n",
    "    q_a_pair = format_qa_pair(q,answer)\n",
    "    q_a_pairs = q_a_pairs + \"\\n---\\n\"+  q_a_pair\n",
    "print(\"Aggregation answer is ...\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decomposition - Method 2, treat sub questions as separated question and \n",
    "#Answer each sub-question individually before passing final RAG Q&A\n",
    "\n",
    "from langchain import hub\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# RAG prompt\n",
    "#prompt_rag = hub.pull(\"rlm/rag-prompt\")\n",
    "#retriever.get_relevant_documents(sub_question)\n",
    "#calling invoke direclty which will call get_relevant_documents\n",
    "\n",
    "'''\n",
    "#Original Codes\n",
    "def retrieve_and_rag(question,prompt_rag,sub_question_generator_chain):\n",
    "    \"\"\"RAG on each sub-question\"\"\"\n",
    "    \n",
    "    # Use our decomposition / \n",
    "    sub_questions = sub_question_generator_chain.invoke({\"question\":question})\n",
    "    # Initialize a list to hold RAG chain results\n",
    "    rag_results = []\n",
    "    \n",
    "    for sub_question in sub_questions:\n",
    "        print(\"subquestion::: \", sub_question)\n",
    "        # Retrieve documents for each sub-question\n",
    "        retrieved_docs = retriever.get_relevant_documents(sub_question)\n",
    "        #Same as Invoke, since Invoke will call the get_relevant_documents evatually \n",
    "        #Use retrieved documents and sub-question separately in RAG chain\n",
    "        answer = (prompt_rag | llm | StrOutputParser()).invoke({\"context\": retrieved_docs, \n",
    "                                                                \"question\": sub_question})\n",
    "        #print (\"answer:::\", answer)\n",
    "        rag_results.append(answer)\n",
    "    \n",
    "    return rag_results,sub_questions\n",
    "'''\n",
    "question=\"How long has each candidate been involved with robotics, and what roles have they held in this field?\"\n",
    "template = \"\"\"You are a helpful assistant that generates multiple sub-questions related to an input question about a resume. \\n\n",
    "The goal is to break down the input question into a set of sub-question about the experience listed in a resume, and each sub-question that can be answers in separately \\n\n",
    "However, you must keep the principle of the origin question in sub-question \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (3 queries):\"\"\"\n",
    "prompt_rag2 = ChatPromptTemplate.from_template(template)\n",
    "llm = ChatOpenAI(temperature=0.2)\n",
    "# ***relaxing temperature between 0 - 0.5 may have a better sub-questions generated.\n",
    "# Chain\n",
    "generate_queries_decomposition2 = ( prompt_rag2 | llm | StrOutputParser() | (lambda x: x.split(\"\\n\")))\n",
    "questions = generate_queries_decomposition2.invoke({\"question\":question})\n",
    "\n",
    "context = \"\"\"Each document (or resume) contains\n",
    "\n",
    "0. The file name at the first line,\n",
    "1. the name, the phone number and the address of a person, \\n\n",
    "2. the education or attending Schools and years of graduation.  \\n\n",
    "3. the work histroy lists all the companies a person worked for.  Each job contains the start and end dates.\n",
    "the end date can be missing since it is current job.  Each job has the job titles, and associated \n",
    "responsibilities or experience of that person. Ecah job is indpendent from other jobs in a document. \n",
    "4. Any awards or social networking information of this person. \\n\n",
    "5. Each job The first job should have the farest year from current date, the last or currnet job has the \n",
    "year closest to the current date.  From the last and first job you should be able to calcuate\n",
    "the total year a person has worked for that job \\n\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "template = \"\"\"Here is the question you need to answer:\n",
    "\n",
    "\\n --- \\n {question} \\n --- \\n\n",
    "\n",
    "Here is additional context relevant to the answer of the question: \n",
    "\n",
    "\\n --- \\n {context} \\n --- \\n\n",
    "Use the above context and any background question + answer pairs to answer the question: \\n {question}\n",
    "\"\"\"\n",
    "\n",
    "#decomposition_prompts by removing the last question and answer pair in the prompt by comparing to the method 1\n",
    "'''Here is any available background question + answer pairs:\n",
    "\n",
    "\\n --- \\n {q_a_pairs} \\n --- \\n\n",
    "'''\n",
    "decomposition_prompt2 = ChatPromptTemplate.from_template(template)\n",
    "answers = []\n",
    "for q in questions:\n",
    "    rag_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | retriever, \n",
    "     \"question\": itemgetter(\"question\") } \n",
    "    | decomposition_prompt2\n",
    "    | llm\n",
    "    | StrOutputParser())\n",
    "\n",
    "    answer = rag_chain.invoke({\"question\":q})\n",
    "    #q_a_pair = format_qa_pair(q,answer)\n",
    "    #q_a_pairs = q_a_pairs + \"\\n---\\n\"+  q_a_pair\n",
    "    #print(\"answer...\", answer)\n",
    "    answers.append(answer)\n",
    "\n",
    "# Wrap the retrieval and RAG process in a RunnableLambda for integration into a chain\n",
    "# with 2nd method, the subquestions generation is the same as the 1st method\n",
    "#answers, questions = retrieve_and_rag(question, prompt_rag, generate_queries_decomposition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decomposition - Method 2 Continue\n",
    "def format_qa_pairs(questions, answers):\n",
    "    \"\"\"Format Q and A pairs\"\"\"\n",
    "    \n",
    "    formatted_string = \"\"\n",
    "    for i, (question, answer) in enumerate(zip(questions, answers), start=1):\n",
    "        formatted_string += f\"Question {i}: {question}\\nAnswer {i}: {answer}\\n\\n\"\n",
    "    return formatted_string.strip()\n",
    "\n",
    "context = format_qa_pairs(questions, answers)\n",
    "\n",
    "template = \"\"\"Here is a set of quest pairs:\n",
    "\n",
    "{context}\n",
    "\n",
    "Use these to synthesize an answer to the question: {question}\n",
    "\"\"\"\n",
    "\n",
    "'''\n",
    "also, can be included following as additional_background \n",
    "the context contains\n",
    "\n",
    "0. The file name at the first line,\n",
    "1. the name, the phone number and the address of a person, \\n\n",
    "2. the education or attending Schools and years of graduation.  \\n\n",
    "3. the work histroy lists all the companies a person worked for.  Each job contains the start and end dates.\n",
    "the end date can be missing since it is current job.  Each job has the job titles, and associated \n",
    "responsibilities or experience of that person. Ecah job is indpendent from other jobs in a document. \n",
    "4. Any awards or social networking information of this person. \\n\n",
    "5. Each job The first job should have the farest year from current date, the last or currnet job has the \n",
    "year closest to the current date.  From the last and first job you should be able to calcuate\n",
    "the total year a person has worked for that job \\n : {question}\n",
    "'''\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag = final_rag_chain.invoke({\"context\":context,\"question\":question})\n",
    "print (\"final_separation_rag is \",  final_rag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query Translation:  step_by_step_decomposition with aggregation and separation as one function\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "import uuid\n",
    "import hashlib\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from operator import itemgetter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "#question = \"Please list all the documents that contains the robotic experience\"\n",
    "question=\"How long has each candidate been involved with robotics, and what roles have they held in this field?\"\n",
    "\n",
    "def extract_pdf_text(file_path):\n",
    "    pdf_file = PdfReader(file_path)\n",
    "    text_data = ''\n",
    "    for pg in pdf_file.pages:\n",
    "        text_data += pg.extract_text()\n",
    "    return text_data\n",
    "\n",
    "def format_qa_pair(question, answer):\n",
    "    \"\"\"Format Q and A pair\"\"\"\n",
    "    \n",
    "    formatted_string = \"\"\n",
    "    formatted_string += f\"Question: {question}\\nAnswer: {answer}\\n\\n\"\n",
    "    #print(\"format string... \", formatted_string, \"\\n\")\n",
    "    return formatted_string.strip()\n",
    "    \n",
    "def format_qa_pairs(questions, answers):\n",
    "    \"\"\"Format Q and A pairs\"\"\"\n",
    "    \n",
    "    formatted_string = \"\"\n",
    "    for i, (question, answer) in enumerate(zip(questions, answers), start=1):\n",
    "        formatted_string += f\"Question {i}: {question}\\nAnswer {i}: {answer}\\n\\n\"\n",
    "    return formatted_string.strip()\n",
    "\n",
    "def step_by_step_decomposition(question, additional_background, selection):\n",
    "    load_dotenv()\n",
    "\n",
    "#if (GCP_PROJECT_ID == None): print (\"Not set\")\n",
    "    OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "    LANGCHAIN_API_KEY = os.getenv('LANGCHAIN_API_KEY')\n",
    "    os.environ['LANGCHAIN_TRACING_V2'] = 'false'  #true for trace\n",
    "    os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "    os.environ[\"LANGCHAIN_PROJECT\"] = \"RAG - Decomposition-Query-Translation\"\n",
    "    #text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "    text_splitter = CharacterTextSplitter()\n",
    "    resume_dir = \".\\\\docs\\\\\"\n",
    "    pdf_text = []\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    vectorstore = Chroma(\"langchain\", embeddings)\n",
    "    for file in os.listdir(resume_dir):\n",
    "        filepath = os.path.join(resume_dir,file)\n",
    "        collection = vectorstore.get('langchain')\n",
    "        if (filepath.endswith('.pdf')): \n",
    "        #Change 2\n",
    "            pdf_text=[]\n",
    "            pdf_text.append (\"File Name: \"+filepath+\"  \\n\"+extract_pdf_text(filepath))\n",
    "            split_docs = text_splitter.create_documents(pdf_text)\n",
    "            existing = vectorstore.get(file)\n",
    "            print(\"adding... \", filepath)\n",
    "\n",
    "            if (existing['ids'] != [] and existing['ids'][0] == file):\n",
    "                print(\"Deleting Duplication .....\", file)\n",
    "                vectorstore.delete(file)\n",
    "            # Add documents back to collection\n",
    "            try:\n",
    "                #print(\"Split Doc   \", split_docs)\n",
    "                #Need to provide IDS list to add_documents, otherwise, it will only pick up the first character of the file name\n",
    "                langchain_ids = vectorstore.add_documents(ids=[file], documents=split_docs) \n",
    "                #print(\"Adding Langchain ID - \", langchain_ids, \" File Name - \", file)\n",
    "                #langchain_ids should be equal to file str\n",
    "            except:\n",
    "                #print(\"Again....Deleting Duplication .....\", file)\n",
    "                vectorstore.delete(file)\n",
    "                #print(\"Existing... \", existing['ids'])\n",
    "                print(\"Can't add.. \", file)\n",
    "                \n",
    "    retriever=vectorstore.as_retriever(max_tokens_limit=10000,\n",
    "                                       collection_metadata={\"hnsw:M\": 1024,\"hnsw:ef\": 64})\n",
    "# Adding search_kwargs or max_tok to avoid the following runtime error\n",
    "# Cannot return the results in a contigious 2D array. Probably ef or M is too small\"\n",
    "# search_kwargs={\"k\": 2}, \n",
    "    template_subquestions = \"\"\"You are a helpful assistant that generates multiple sub-questions related to an input question about a resume. \\n\n",
    "The goal is to break down the input question into a set of sub-question about the experience listed in a resume, and each sub-question that can be answers in separately \\n\n",
    "However, you must keep the principle of the origin question in sub-question \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (3 queries):\"\"\"\n",
    "\n",
    "    prompt_decomposition = ChatPromptTemplate.from_template(template_subquestions)\n",
    "\n",
    "    #llm = ChatOpenAI(temperature=0.2)\n",
    "    # ***relaxing temperature between 0 - 0.5 may have a better sub-questions generated.\n",
    "\n",
    "    # Chain\n",
    "    #generate_queries_decomposition = ( prompt_decomposition | llm | StrOutputParser() | (lambda x: x.split(\"\\n\")))\n",
    "\n",
    "    \n",
    "    prompt_decomposition_output =  prompt_decomposition.invoke(question)\n",
    "    #print(\"Output after prompt_perspectives:\", prompt_decomposition_output)\n",
    "\n",
    "    chat_openai_output = ChatOpenAI(temperature=0.2)(prompt_decomposition_output)\n",
    "        ##print(\"Output after ChatOpenAI:\", chat_openai_output)\n",
    "    \n",
    "    parser_output = StrOutputParser().invoke(chat_openai_output)\n",
    "        #print(\"output after StrOutputParser:\", parser_output)\n",
    "        #str_output_parser_output = output_parser(chat_openai_output)\n",
    "        #print(\"Output after StrOutputParser:\", str_output_parser_output)\n",
    "    questions = parser_output.split(\"\\n\")\n",
    "    #print(\"questions len...\", len(questions), \"...\", questions)\n",
    "    \n",
    "\n",
    "    template = \"\"\"Here is the question you need to answer:\n",
    "\n",
    "    \\n --- \\n {question} \\n --- \\n\n",
    "\n",
    "    Here is any available background question + answer pairs:\n",
    "\n",
    "    \\n --- \\n {q_a_pairs} \\n --- \\n\n",
    "\n",
    "    Here is additional background \\n --- \\n {background}\n",
    "    and context relevant to the answer of the question: \n",
    "\n",
    "    \\n --- \\n {context} \\n --- \\n\n",
    "    Use the above context and any background question + answer pairs to answer the question: \\n {question}\n",
    "    \"\"\"\n",
    "\n",
    "    decomposition_prompt = ChatPromptTemplate.from_template(template)\n",
    "    \n",
    "    #Decomposition - Method 1 Continue\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# llm\n",
    "    llm = ChatOpenAI(temperature=0.2)\n",
    "    answers = []\n",
    "    q_a_pairs = \"\"\n",
    "    for q in questions:\n",
    "        print(\"question... \", q, \"\\n\")\n",
    "        docs = retriever.invoke(q)\n",
    "        print(\"docs... \", docs)\n",
    "        if (selection == 1):\n",
    "            aggregations = decomposition_prompt.invoke({\"question\": q,\n",
    "                                                        \"q_a_pairs\": q_a_pairs, \n",
    "                                                        \"context\": docs,\n",
    "                                                        \"background\": additional_background})\n",
    "        else:\n",
    "            q_a_pairs = \"\"\n",
    "            aggregations = decomposition_prompt.invoke({\"question\": q,\n",
    "                                                        \"q_a_pairs\": q_a_pairs,\n",
    "                                                        \"context\": docs,\n",
    "                                                        \"background\": additional_background})\n",
    "            \n",
    "        chat_openai_output = llm(aggregations)\n",
    "        parser_output = StrOutputParser().invoke(chat_openai_output)\n",
    "        #print(\"Parser Output \", parser_output)\n",
    "        if (selection == 1): \n",
    "            final_rag1 = parser_output\n",
    "            q_a_pair = format_qa_pair(q, parser_output)\n",
    "            q_a_pairs = q_a_pairs + \"\\n\" + q_a_pair\n",
    "        else:        \n",
    "            answers.append(parser_output)\n",
    "            \n",
    "        \n",
    "    if (selection != 1):\n",
    "        context = format_qa_pairs(questions, answers)\n",
    "        #print(\"context....\", context)\n",
    "        template = \"\"\"Here is a set of quest pairs:\n",
    "\n",
    "        {context}\n",
    "\n",
    "        Use these to synthesize an answer to the question: {question}\n",
    "        \"\"\"\n",
    "        prompt = ChatPromptTemplate.from_template(template)\n",
    "        separation = prompt.invoke({\"question\": question, \"context\": context})\n",
    "        chat_openai_output = llm(separation)\n",
    "        final_rag2 = StrOutputParser().invoke(chat_openai_output)\n",
    "        print(\"\\n\\n\\nFinal Rag - Separation answer is:    \", final_rag2)\n",
    "    else:\n",
    "        print(\"\\n\\n\\nFinal Rag - Aggregation answer is:    \", final_rag1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "additional_background = \"\"\"Each document (or resume) contains\n",
    "\n",
    "    0. The file name at the first line,\n",
    "    1. the name, the phone number and the address of a person, \\n\n",
    "    2. the education or attending Schools and years of graduation.  \\n\n",
    "    3. the work histroy lists all the companies a person worked for.  Each job contains the start and end dates.\n",
    "    the end date can be missing since it is current job.  Each job has the job titles, and associated \n",
    "    responsibilities or experience of that person. Ecah job is indpendent from other jobs in a document. \n",
    "    4. Any awards or social networking information of this person. \\n\n",
    "    5. Each job The first job should have the farest year from current date, the last or currnet job has the \n",
    "    year closest to the current date.  From the last and first job you should be able to calcuate\n",
    "    the total year a person has worked for that job \\n\n",
    "    \n",
    "    \n",
    "\"\"\"\n",
    "'''\n",
    "\n",
    "additional_background = \"\"\n",
    "question=\"How long has each candidate been involved with robotics, and what roles have they held in this field?\"\n",
    "step_by_step_decomposition(question, additional_background, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_background = \"\"\"Each document (or resume) contains\n",
    "\n",
    "    0. The file name at the first line,\n",
    "    1. the name, the phone number and the address of a person, \\n\n",
    "    2. the education or attending Schools and years of graduation.  \\n\n",
    "    3. the work histroy lists all the companies a person worked for.  Each job contains the start and end dates.\n",
    "    the end date can be missing since it is current job.  Each job has the job titles, and associated \n",
    "    responsibilities or experience of that person. Ecah job is indpendent from other jobs in a document. \n",
    "    4. Any awards or social networking information of this person. \\n\n",
    "    5. Each job The first job should have the farest year from current date, the last or currnet job has the \n",
    "    year closest to the current date.  From the last and first job you should be able to calcuate\n",
    "    the total year a person has worked for that job \\n\n",
    "    \n",
    "    \n",
    "\"\"\"\n",
    "additional_background = \"\"\n",
    "question=\"Who have the Java experience?\"\n",
    "step_by_step_decomposition(question, additional_background, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question=\"Please provide summary of all candidates\"\n",
    "step_by_step_decomposition(question, context, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleanup the vectordb\n",
    "def clean_vectorstore():\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    ectorstore = Chroma(\"langchain\", embeddings)\n",
    "    ids = vectorstore.get().get('ids')\n",
    "    len(ids)\n",
    "    print(\"ids \", ids)\n",
    "    for id in ids: \n",
    "        docs = vectorstore.get(id)\n",
    "        print(\"docs \", docs)\n",
    "        print(id)\n",
    "        vectorstore.delete(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_vectorstore()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cpu",
   "language": "python",
   "name": "cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
