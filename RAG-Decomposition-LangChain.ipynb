{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "RAG - Decomposition question to sub-questions and aggregating each sub-question/answar pair\n",
    "as part of prompt context in the next sub-question prompt till final.\n",
    "\n",
    "RAG - Decomposition question to sub-questions, and provide the final answer from the \n",
    "retrieved documents of each sub-question.\n",
    "\n",
    "Decomposition translation is different from multi-query or fusion, instead of \n",
    "creating similar questions, it creates sub-questions from the original question. \n",
    "\n",
    "step_to_step_decomposition, at end, is based on the LangChain Rag youtube tutorial below, \n",
    "the code from the tutorial is listed in the first half of the ipython notebook. \n",
    "\n",
    "This function follows the same logic from the tutorial but without the abstract of \n",
    "Chain/Pipeline. This approach helps me to understand the sub-questions creation, \n",
    "llm processes and also can improve the final prompt preparation. \n",
    "\n",
    "Of course, the langsmith is a great tool as well. \n",
    "\n",
    "I may be wrong, but it seems to me the separate the sub-questions as individual questions\n",
    "can return more accurate results than the aggregation approach. \n",
    "However, the aggregation decomposition approach provide more details in answer.\n",
    "\n",
    "\n",
    "Input arguments to the step_to_step_fusion are\n",
    "1. Quesition: question string\n",
    "2. Additional_background:  the helping instructions you want the ChatPGT to return the Q&A \n",
    "with a better answer.\n",
    "3. Aggregation: 1 means aggregating sub-questions as prompt for the next sub-question Q&A\n",
    "\n",
    "\n",
    "Please review the RAG tutorial from Langchain in details (part 7 \n",
    "- decompistion query translation)\n",
    "https://www.youtube.com/watch?v=h0OPWlEOank\n",
    "\n",
    "'''\n",
    "#RAG-- Common Rag\n",
    "#Query Translation:   Multi-Query, Fusion and Decompsoition \n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "#if (GCP_PROJECT_ID == None): print (\"Not set\")\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "LANGCHAIN_API_KEY = os.getenv('LANGCHAIN_API_KEY')\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'false'  #true for trace\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"RAG - Decomposition-Query-Translation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "#RAG-- Common Rag\n",
    "#Query Translation:   Multi-Query, Fusion and Decompsoition \n",
    "from langchain import hub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "#text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "text_splitter = CharacterTextSplitter()\n",
    "question = \"Please list all the documents that contains the robotic experience\"\n",
    "question=\"List all candidates by name that who have Java work experience?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RAG-- Common Rag\n",
    "#Query Translation:   Multi-Query, Fusion and Decompsoition \n",
    "#Loading PDF resumes from local instead of the Web\n",
    "\n",
    "import uuid\n",
    "import hashlib\n",
    "from PyPDF2 import PdfReader\n",
    "def extract_pdf_text(file_path):\n",
    "    pdf_file = PdfReader(file_path)\n",
    "    text_data = ''\n",
    "    for pg in pdf_file.pages:\n",
    "        text_data += pg.extract_text()\n",
    "    return text_data\n",
    "\n",
    "\n",
    "resume_dir = \".\\\\docs\\\\\"\n",
    "pdf_text = []\n",
    "\n",
    "\n",
    "#Change 1\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = Chroma(\"langchain\", embeddings)\n",
    "\n",
    "\n",
    "\n",
    "def create_uuid_from_string(val: str):\n",
    "    hex_string = hashlib.md5(val.encode(\"UTF-8\")).hexdigest()\n",
    "    return uuid.UUID(hex=hex_string)\n",
    "\n",
    "\n",
    "for file in os.listdir(resume_dir):\n",
    "    filepath = os.path.join(resume_dir,file)\n",
    "    collection = vectorstore.get('langchain')\n",
    "    if (filepath.endswith('.pdf')): \n",
    "        #Change 2\n",
    "        pdf_text=[]\n",
    "        pdf_text.append (\"File Name: \"+filepath+\"  \\n\"+extract_pdf_text(filepath))\n",
    "        split_docs = text_splitter.create_documents(pdf_text)\n",
    "        existing = vectorstore.get(file)\n",
    "       \n",
    "        if (existing['ids'] != [] and existing['ids'][0] == file):\n",
    "            print(\"Deleting Duplication .....\", file)\n",
    "            vectorstore.delete(file)\n",
    "        # Add documents back to collection\n",
    "        try:\n",
    "            #print(\"Split Doc   \", split_docs)\n",
    "            #Need to provide IDS list to add_documents, otherwise, it will only pick up the first character of the file name\n",
    "            langchain_ids = vectorstore.add_documents(ids=[file], documents=split_docs) \n",
    "            print(\"Adding Langchain ID - \", langchain_ids, \" File Name - \", file)\n",
    "            #langchain_ids should be equal to file str\n",
    "        except:\n",
    "            #print(\"Again....Deleting Duplication .....\", file)\n",
    "            vectorstore.delete(file)\n",
    "            #print(\"Existing... \", existing['ids'])\n",
    "            print(\"Can't add.. \", file)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieving\n",
    "#RAG-- Common Rag\n",
    "\n",
    "#vectorstore = Chroma.from_documents(documents=split_docs, embedding = embedding)\n",
    "#retriever=vectorstore.as_retriever(search_kwargs={\"k\": 2}, max_tokens_limit=10000)\n",
    "retriever=vectorstore.as_retriever(search_kwargs={\"k\": 2}, max_tokens_limit=10000,\n",
    "                                   collection_metadata={\"hnsw:M\": 1024,\"hnsw:ef\": 64})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decomposition - Method 1, Aggreating subquestions.\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "#question = \"Please list all the documents that contains the robotic experience\"\n",
    "#Decomposition\n",
    "template = \"\"\"You are a helpful assistant that generates multiple sub-questions related to an input question about a resume. \\n\n",
    "The goal is to break down the input question into a set of sub-question about the experience listed in a resume, and each sub-question that can be answers in separately \\n\n",
    "However, you must keep the principle of the origin question in sub-question \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (3 queries):\"\"\"\n",
    "prompt_decomposition = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decomposition - Method 1  Continue\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "#question=\"How long has each candidate been involved with robotics, and what roles have they held in this field?\"\n",
    "#question =\"Please list all the candidate name that who has the robotic experience\"\n",
    "# LLM\n",
    "question=\"List all candidates by name that who have Java work experience?\"\n",
    "llm = ChatOpenAI(temperature=0.2)\n",
    "# ***relaxing temperature between 0 - 0.5 may have a better sub-questions generated.\n",
    "# Chain\n",
    "generate_queries_decomposition = ( prompt_decomposition | llm | StrOutputParser() | (lambda x: x.split(\"\\n\")))\n",
    "# Run\n",
    "questions = generate_queries_decomposition.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decomposition - Method 1 Continue\n",
    "\n",
    "context = \"\"\"Each document (or resume) contains\n",
    "\n",
    "0. The file name is listed in the first line,\n",
    "1. The resume contains the name, the phone number and the address of a person, \\n\n",
    "2. The resume contains the degree and attending university and years of graduation.  \\n\n",
    "3. The resume contains the work history of all the companies a person worked for.  \n",
    "Each job may contain the start and end dates, and job title. If the end date is \n",
    "missing then, the job is the current job.  \n",
    "4. Each job has the job titles, and associated responsibilities or experience of \n",
    "that person. Each job is independent from other jobs in the same document. \n",
    "5. The resume may contain any awards or social networking information of this person. \\n\n",
    "6. The first job should have the farest year from current date, the last or current job \n",
    "has the closest to the current date.  From the last and first job you should be able \n",
    "to calculate the total year a person has worked for each job and total years of a person\n",
    "has been worked.\\n\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "template = \"\"\"Here is the question you need to answer:\n",
    "\n",
    "\\n --- \\n {question} \\n --- \\n\n",
    "\n",
    "Here is any available background question + answer pairs:\n",
    "\n",
    "\\n --- \\n {q_a_pairs} \\n --- \\n\n",
    "\n",
    "Here is additional context relevant to the answer of the question: \n",
    "\n",
    "\\n --- \\n {context} \\n --- \\n\n",
    "\n",
    "Use the above context and any background question + answer pairs to answer the question: \\n {question}\n",
    "\"\"\"\n",
    "\n",
    "decomposition_prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decomposition - Method 1 Continue\n",
    "from operator import itemgetter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def format_qa_pair(question, answer):\n",
    "    \"\"\"Format Q and A pair\"\"\"\n",
    "    formatted_string = \"\"\n",
    "    formatted_string += f\"Question: {question}\\nAnswer: {answer}\\n\\n\"\n",
    "    #print(\"format String...\", formatted_string.strip(), \"\\n\")\n",
    "    return formatted_string.strip()\n",
    "\n",
    "# llm\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "q_a_pairs = \"\"\n",
    "\n",
    "for q in questions:\n",
    "    rag_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | retriever, \n",
    "     \"question\": itemgetter(\"question\"),\n",
    "     \"q_a_pairs\": itemgetter(\"q_a_pairs\")} \n",
    "    | decomposition_prompt\n",
    "    | llm\n",
    "    | StrOutputParser())\n",
    "\n",
    "    answer = rag_chain.invoke({\"question\":q,\"q_a_pairs\":q_a_pairs})\n",
    "    q_a_pair = format_qa_pair(q,answer)\n",
    "    q_a_pairs = q_a_pairs + \"\\n---\\n\"+  q_a_pair\n",
    "print(\"Aggregation answer is ...\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decomposition - Method 2, treat sub questions as separated question and \n",
    "#Answer each sub-question individually before passing final RAG Q&A\n",
    "\n",
    "from langchain import hub\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# RAG prompt\n",
    "#prompt_rag = hub.pull(\"rlm/rag-prompt\")\n",
    "#retriever.get_relevant_documents(sub_question)\n",
    "#calling invoke direclty which will call get_relevant_documents\n",
    "\n",
    "'''\n",
    "#Original Codes\n",
    "def retrieve_and_rag(question,prompt_rag,sub_question_generator_chain):\n",
    "    \"\"\"RAG on each sub-question\"\"\"\n",
    "    \n",
    "    # Use our decomposition / \n",
    "    sub_questions = sub_question_generator_chain.invoke({\"question\":question})\n",
    "    # Initialize a list to hold RAG chain results\n",
    "    rag_results = []\n",
    "    \n",
    "    for sub_question in sub_questions:\n",
    "        print(\"subquestion::: \", sub_question)\n",
    "        # Retrieve documents for each sub-question\n",
    "        retrieved_docs = retriever.get_relevant_documents(sub_question)\n",
    "        #Same as Invoke, since Invoke will call the get_relevant_documents evatually \n",
    "        #Use retrieved documents and sub-question separately in RAG chain\n",
    "        answer = (prompt_rag | llm | StrOutputParser()).invoke({\"context\": retrieved_docs, \n",
    "                                                                \"question\": sub_question})\n",
    "        #print (\"answer:::\", answer)\n",
    "        rag_results.append(answer)\n",
    "    \n",
    "    return rag_results,sub_questions\n",
    "'''\n",
    "question=\"How long has each candidate been involved with robotics, and what roles have they held in this field?\"\n",
    "template = \"\"\"You are a helpful assistant that generates multiple sub-questions related to an input question about a resume. \\n\n",
    "The goal is to break down the input question into a set of sub-question about the experience listed in a resume, and each sub-question that can be answers in separately \\n\n",
    "However, you must keep the principle of the origin question in sub-question \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (3 queries):\"\"\"\n",
    "prompt_rag2 = ChatPromptTemplate.from_template(template)\n",
    "llm = ChatOpenAI(temperature=0.2)\n",
    "# ***relaxing temperature between 0 - 0.5 may have a better sub-questions generated.\n",
    "# Chain\n",
    "generate_queries_decomposition2 = ( prompt_rag2 | llm | StrOutputParser() | (lambda x: x.split(\"\\n\")))\n",
    "questions = generate_queries_decomposition2.invoke({\"question\":question})\n",
    "\n",
    "context = \"\"\"Each document (or resume) contains\n",
    "\n",
    "0. The file name is listed in the first line,\n",
    "1. The resume contains the name, the phone number and the address of a person, \\n\n",
    "2. The resume contains the degree and attending university and years of graduation.  \\n\n",
    "3. The resume contains the work history of all the companies a person worked for.  \n",
    "Each job may contain the start and end dates, and job title. If the end date is \n",
    "missing then, the job is the current job.  \n",
    "4. Each job has the job titles, and associated responsibilities or experience of \n",
    "that person. Each job is independent from other jobs in the same document. \n",
    "5. The resume may contain any awards or social networking information of this person. \\n\n",
    "6. The first job should have the farest year from current date, the last or current job \n",
    "has the closest to the current date.  From the last and first job you should be able \n",
    "to calculate the total year a person has worked for each job and total years of a person\n",
    "has been worked.\\n\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "template = \"\"\"Here is the question you need to answer:\n",
    "\n",
    "\\n --- \\n {question} \\n --- \\n\n",
    "\n",
    "Here is additional context relevant to the answer of the question: \n",
    "\n",
    "\\n --- \\n {context} \\n --- \\n\n",
    "Use the above context and any background question + answer pairs to answer the question: \\n {question}\n",
    "\"\"\"\n",
    "\n",
    "#decomposition_prompts by removing the last question and answer pair in the prompt by comparing to the method 1\n",
    "'''Here is any available background question + answer pairs:\n",
    "\n",
    "\\n --- \\n {q_a_pairs} \\n --- \\n\n",
    "'''\n",
    "decomposition_prompt2 = ChatPromptTemplate.from_template(template)\n",
    "answers = []\n",
    "for q in questions:\n",
    "    rag_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | retriever, \n",
    "     \"question\": itemgetter(\"question\") } \n",
    "    | decomposition_prompt2\n",
    "    | llm\n",
    "    | StrOutputParser())\n",
    "\n",
    "    answer = rag_chain.invoke({\"question\":q})\n",
    "    #q_a_pair = format_qa_pair(q,answer)\n",
    "    #q_a_pairs = q_a_pairs + \"\\n---\\n\"+  q_a_pair\n",
    "    #print(\"answer...\", answer)\n",
    "    answers.append(answer)\n",
    "\n",
    "# Wrap the retrieval and RAG process in a RunnableLambda for integration into a chain\n",
    "# with 2nd method, the subquestions generation is the same as the 1st method\n",
    "#answers, questions = retrieve_and_rag(question, prompt_rag, generate_queries_decomposition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decomposition - Method 2 Continue\n",
    "def format_qa_pairs(questions, answers):\n",
    "    \"\"\"Format Q and A pairs\"\"\"\n",
    "    \n",
    "    formatted_string = \"\"\n",
    "    for i, (question, answer) in enumerate(zip(questions, answers), start=1):\n",
    "        formatted_string += f\"Question {i}: {question}\\nAnswer {i}: {answer}\\n\\n\"\n",
    "    return formatted_string.strip()\n",
    "\n",
    "context = format_qa_pairs(questions, answers)\n",
    "\n",
    "template = \"\"\"Here is a set of quest pairs:\n",
    "\n",
    "{context}\n",
    "\n",
    "Use these to synthesize an answer to the question: {question}\n",
    "\"\"\"\n",
    "\n",
    "'''\n",
    "also, can be included following as additional_background \n",
    "the context contains\n",
    "0. The file name is listed in the first line,\n",
    "1. The resume contains the name, the phone number and the address of a person, \\n\n",
    "2. The resume contains the degree and attending university and years of graduation.  \\n\n",
    "3. The resume contains the work history of all the companies a person worked for.  \n",
    "Each job may contain the start and end dates, and job title. If the end date is \n",
    "missing then, the job is the current job.  \n",
    "4. Each job has the job titles, and associated responsibilities or experience of \n",
    "that person. Each job is independent from other jobs in the same document. \n",
    "5. The resume may contain any awards or social networking information of this person. \\n\n",
    "6. The first job should have the farest year from current date, the last or current job \n",
    "has the closest to the current date.  From the last and first job you should be able \n",
    "to calculate the total year a person has worked for each job and total years of a person\n",
    "has been worked.\\n : {question}\n",
    "'''\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag = final_rag_chain.invoke({\"context\":context,\"question\":question})\n",
    "print (\"final_separation_rag is \",  final_rag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query Translation:  step_by_step_decomposition with aggregation and separation as one function\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "import uuid\n",
    "import hashlib\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from operator import itemgetter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "#question = \"Please list all the documents that contains the robotic experience\"\n",
    "#question=\"How long has each candidate been involved with robotics, and what roles have they held in this field?\"\n",
    "\n",
    "def extract_pdf_text(file_path):\n",
    "    pdf_file = PdfReader(file_path)\n",
    "    text_data = ''\n",
    "    for pg in pdf_file.pages:\n",
    "        text_data += pg.extract_text()\n",
    "    return text_data\n",
    "\n",
    "def format_qa_pair(question, answer):\n",
    "    \"\"\"Format Q and A pair\"\"\"\n",
    "    \n",
    "    formatted_string = \"\"\n",
    "    formatted_string += f\"Question: {question}\\nAnswer: {answer}\\n\\n\"\n",
    "    #print(\"format string... \", formatted_string, \"\\n\")\n",
    "    return formatted_string.strip()\n",
    "    \n",
    "def format_qa_pairs(questions, answers):\n",
    "    \"\"\"Format Q and A pairs\"\"\"\n",
    "    \n",
    "    formatted_string = \"\"\n",
    "    for i, (question, answer) in enumerate(zip(questions, answers), start=1):\n",
    "        formatted_string += f\"Question {i}: {question}\\nAnswer {i}: {answer}\\n\\n\"\n",
    "    return formatted_string.strip()\n",
    "\n",
    "def step_by_step_decomposition(question, additional_background, selection):\n",
    "    load_dotenv()\n",
    "\n",
    "#if (GCP_PROJECT_ID == None): print (\"Not set\")\n",
    "    OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "    LANGCHAIN_API_KEY = os.getenv('LANGCHAIN_API_KEY')\n",
    "    os.environ['LANGCHAIN_TRACING_V2'] = 'false'  #true for trace\n",
    "    os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "    os.environ[\"LANGCHAIN_PROJECT\"] = \"RAG - Decomposition-Query-Translation\"\n",
    "    #text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "    text_splitter = CharacterTextSplitter()\n",
    "    resume_dir = \".\\\\docs\\\\\"\n",
    "    pdf_text = []\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    vectorstore = Chroma(\"langchain\", embeddings)\n",
    "    for file in os.listdir(resume_dir):\n",
    "        filepath = os.path.join(resume_dir,file)\n",
    "        collection = vectorstore.get('langchain')\n",
    "        if (filepath.endswith('.pdf')): \n",
    "        #Change 2\n",
    "            pdf_text=[]\n",
    "            pdf_text.append (\"File Name: \"+filepath+\"  \\n\"+extract_pdf_text(filepath))\n",
    "            split_docs = text_splitter.create_documents(pdf_text)\n",
    "            existing = vectorstore.get(file)\n",
    "            #print(\"adding... \", filepath)\n",
    "\n",
    "            if (existing['ids'] != [] and existing['ids'][0] == file):\n",
    "                #print(\"Deleting Duplication .....\", file)\n",
    "                vectorstore.delete(file)\n",
    "            # Add documents back to collection\n",
    "            try:\n",
    "                #print(\"Split Doc   \", split_docs)\n",
    "                #Need to provide IDS list to add_documents, otherwise, it will only pick up the first character of the file name\n",
    "                langchain_ids = vectorstore.add_documents(ids=[file], documents=split_docs) \n",
    "                #print(\"Adding Langchain ID - \", langchain_ids, \" File Name - \", file)\n",
    "                #langchain_ids should be equal to file str\n",
    "            except:\n",
    "                #print(\"Again....Deleting Duplication .....\", file)\n",
    "                vectorstore.delete(file)\n",
    "                #print(\"Existing... \", existing['ids'])\n",
    "                print(\"Can't add.. \", file)\n",
    "                \n",
    "    retriever=vectorstore.as_retriever(max_tokens_limit=20000,\n",
    "                                       collection_metadata={\"hnsw:M\": 1024,\"hnsw:ef\": 64})\n",
    "# Adding search_kwargs or max_tok to avoid the following runtime error\n",
    "# Cannot return the results in a contigious 2D array. Probably ef or M is too small\"\n",
    "# search_kwargs={\"k\": 2}, \n",
    "    template_subquestions = \"\"\"You are a helpful assistant that generates multiple sub-questions related to an input question about a resume. \\n\n",
    "The goal is to break down the input question into a set of sub-question about the experience listed in a resume, and each sub-question that can be answers in separately \\n\n",
    "However, you must keep the principle of the origin question in sub-question \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (3 queries):\"\"\"\n",
    "\n",
    "    prompt_decomposition = ChatPromptTemplate.from_template(template_subquestions)\n",
    "\n",
    "    #llm = ChatOpenAI(temperature=0.2)\n",
    "    # ***relaxing temperature between 0 - 0.5 may have a better sub-questions generated.\n",
    "\n",
    "    # Chain\n",
    "    #generate_queries_decomposition = ( prompt_decomposition | llm | StrOutputParser() | (lambda x: x.split(\"\\n\")))\n",
    "\n",
    "    \n",
    "    prompt_decomposition_output =  prompt_decomposition.invoke(question)\n",
    "    #print(\"Output after prompt_perspectives:\", prompt_decomposition_output)\n",
    "\n",
    "    chat_openai_output = ChatOpenAI(temperature=0.2)(prompt_decomposition_output)\n",
    "        ##print(\"Output after ChatOpenAI:\", chat_openai_output)\n",
    "    \n",
    "    parser_output = StrOutputParser().invoke(chat_openai_output)\n",
    "        #print(\"output after StrOutputParser:\", parser_output)\n",
    "        #str_output_parser_output = output_parser(chat_openai_output)\n",
    "        #print(\"Output after StrOutputParser:\", str_output_parser_output)\n",
    "    questions = parser_output.split(\"\\n\")\n",
    "    #print(\"questions len...\", len(questions), \"...\", questions)\n",
    "    \n",
    "\n",
    "    template = \"\"\"Here is the question you need to answer:\n",
    "\n",
    "    \\n --- \\n {question} \\n --- \\n\n",
    "\n",
    "    Here is any available background question + answer pairs:\n",
    "\n",
    "    \\n --- \\n {q_a_pairs} \\n --- \\n\n",
    "\n",
    "    Here is additional background \\n --- \\n {background}\n",
    "    and context relevant to the answer of the question: \n",
    "\n",
    "    \\n --- \\n {context} \\n --- \\n\n",
    "    Use the above context and any background question + answer pairs to answer the question: \\n {question}\n",
    "    \"\"\"\n",
    "\n",
    "    decomposition_prompt = ChatPromptTemplate.from_template(template)\n",
    "    \n",
    "    #Decomposition - Method 1 Continue\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# llm\n",
    "    llm = ChatOpenAI(temperature=0.2)\n",
    "    answers = []\n",
    "    q_a_pairs = \"\"\n",
    "    for q in questions:\n",
    "        #print(\"question... \", q, \"\\n\")\n",
    "        docs = retriever.invoke(q)\n",
    "        #print(\"docs... \", docs)\n",
    "        if (selection == 1):\n",
    "            aggregations = decomposition_prompt.invoke({\"question\": q,\n",
    "                                                        \"q_a_pairs\": q_a_pairs, \n",
    "                                                        \"context\": docs,\n",
    "                                                        \"background\": additional_background})\n",
    "        else:\n",
    "            q_a_pairs = \"\"\n",
    "            aggregations = decomposition_prompt.invoke({\"question\": q,\n",
    "                                                        \"q_a_pairs\": q_a_pairs,\n",
    "                                                        \"context\": docs,\n",
    "                                                        \"background\": additional_background})\n",
    "            \n",
    "        chat_openai_output = llm(aggregations)\n",
    "        parser_output = StrOutputParser().invoke(chat_openai_output)\n",
    "        #print(\"Parser Output \", parser_output)\n",
    "        if (selection == 1): \n",
    "            final_rag1 = parser_output\n",
    "            q_a_pair = format_qa_pair(q, parser_output)\n",
    "            q_a_pairs = q_a_pairs + \"\\n\" + q_a_pair\n",
    "        else:        \n",
    "            answers.append(parser_output)\n",
    "            \n",
    "        \n",
    "    if (selection != 1):\n",
    "        context = format_qa_pairs(questions, answers)\n",
    "        #print(\"context....\", context)\n",
    "        template = \"\"\"Here is a set of quest pairs:\n",
    "\n",
    "        {context}\n",
    "\n",
    "        Use these to synthesize an answer to the question: {question}\n",
    "        \"\"\"\n",
    "        prompt = ChatPromptTemplate.from_template(template)\n",
    "        separation = prompt.invoke({\"question\": question, \"context\": context})\n",
    "        chat_openai_output = llm(separation)\n",
    "        final_rag2 = StrOutputParser().invoke(chat_openai_output)\n",
    "        print(\"\\n\\n\\nFinal Rag - Separation answer is:    \", final_rag2)\n",
    "    else:\n",
    "        print(\"\\n\\n\\nFinal Rag - Aggregation answer is:    \", final_rag1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_background = \"\"\"Each document (or resume) may include\n",
    "0. The file name is listed in the first line,\n",
    "1. The resume contains the name, the phone number and the address of a person, \\n\n",
    "2. The resume contains the degree and attending university and years of graduation.  \\n\n",
    "3. The resume contains the work history of all the companies a person worked for.  \n",
    "Each job may contain the start and end dates, and job title. If the end date is \n",
    "missing then, the job is the current job.  \n",
    "4. Each job has the job titles, and associated responsibilities or experience of \n",
    "that person. Each job is independent from other jobs in the same document. \n",
    "5. The resume may contain any awards or social networking information of this person. \\n\n",
    "6. The first job should have the farest year from current date, the last or current job \n",
    "has the closest to the current date.  From the last and first job you should be able \n",
    "to calculate the total year a person has worked for each job and total years of a person\n",
    "has been worked.\\n\n",
    "    \n",
    "    \n",
    "\"\"\"\n",
    "additional_background = \"\"\n",
    "question=\"Who have the Java experience?\"\n",
    "step_by_step_decomposition(question, additional_background, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Final Rag - Aggregation answer is:     Based on the provided background information, the candidate Teresa Technical has worked on Java projects during her Software Development Internship at Pros in Denver, CO. She designed a REST API call in Java, used a topological sorting algorithm on a DAG, and refactored existing API calls to use XML Serializable objects and new API backend methods. Additionally, she has listed proficiency in Java as one of her technical skills. Therefore, Teresa Technical can provide examples of Java code she has written or contributed to based on her experience at Pros.\n",
    "\n",
    "\n",
    "Final Rag - Separation answer is:     Based on the provided information, both Teresa Technical and Sam C. Yang have significant experience with Java programming. Teresa Technical has completed various projects involving Java, such as designing REST API calls, implementing algorithms for data dependency resolution, and refactoring existing API calls. On the other hand, Sam C. Yang has over 20 years of experience in software development and program management, which includes experience in Java programming. Therefore, both individuals have demonstrated their expertise and proficiency in Java through their professional experiences and accomplishments.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question=\"Please provide summary of all candidates\"\n",
    "step_by_step_decomposition(question, context, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Final Rag - Aggregation answer is:     Candidate 1 (Sam C. Yang) has a Master of Science in Computer Engineering from the University of Southern California and a Bachelor of Science in Physics from F. J. U in Taiwan. This educational background provides him with a strong foundation in both computer engineering and physics, which are relevant to the technical aspects of the position. Additionally, his extensive experience in software development and program management aligns well with the requirements of the role.\n",
    "\n",
    "Candidate 2 (Teresa Technical) holds a Bachelor of Science in Computer Science from The University of Texas at Austin, with relevant coursework in Object-Oriented Programming, Artificial Intelligence, Algorithms, Data Structures, and Data Mining. This educational background equips her with the necessary technical skills and knowledge to excel in the position. Her experience as a Software Development Intern and Online Marketing Intern further demonstrates her practical application of these skills.\n",
    "\n",
    "Candidate 3 (Daisuke Data Science) has a Bachelor of Science in Biology with a focus on Computational Biology from The University of Texas at Austin. This educational background provides him with a strong foundation in biology and computational science, which are relevant to the data science aspects of the position. His experience as a Data Science Intern at Merck & Co. and as a Data Analyst Intern at Austin Sports Medicine, Orthopedics showcases his practical experience in utilizing data analytics tools and techniques.\n",
    "\n",
    "Overall, the educational backgrounds of all three candidates contribute to their suitability for the position by providing them with the necessary knowledge, skills, and experience to excel in their respective roles.\n",
    "\n",
    "Final Rag - Separation answer is:     Summary of all candidates:\n",
    "\n",
    "Candidate 1 (Sam C. Yang) brings over 20 years of software development and program management experience in various industries, with a strong focus on SaaS/Cloud, Robotics/Sensors/Control, and E-Commerce. His extensive experience in managing cross-functional teams and delivering products from RFP/RFQ to Mass Production makes him a valuable asset for the position.\n",
    "\n",
    "Candidate 2 (Teresa Technical) holds a Bachelor of Science in Computer Science and has practical experience as a Software Development Intern and Online Marketing Intern. Her proficiency in Java and familiarity with other programming languages, coupled with her research in computational science and engineering, showcases her technical skills and ability to apply them in real-world projects.\n",
    "\n",
    "Candidate 3 (Daisuke Data Science) has a background in Biology with a focus on Computational Biology, providing him with a strong foundation in data analysis and statistical modeling. His internship experiences in data science and healthcare analytics, along with his research projects in bioinformatics, demonstrate his ability to leverage data for insights and decision-making.\n",
    "\n",
    "Overall, each candidate brings a unique set of qualifications, skills, and experiences that can contribute to the company's success in different ways. Sam C. Yang's leadership and program management expertise, Teresa Technical's technical proficiency and research background, and Daisuke Data Science's data analysis skills and research experience make them all strong contenders for the position\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "additional_background = \"\"\"Each document (or resume) may include\n",
    "\n",
    "    0. The file name is listed in the first line,\n",
    "1. The resume contains the name, the phone number and the address of a person, \\n\n",
    "2. The resume contains the degree and attending university and years of graduation.  \\n\n",
    "3. The resume contains the work history of all the companies a person worked for.  \n",
    "Each job may contain the start and end dates, and job title. If the end date is \n",
    "missing then, the job is the current job.  \n",
    "4. Each job has the job titles, and associated responsibilities or experience of \n",
    "that person. Each job is independent from other jobs in the same document. \n",
    "5. The resume may contain any awards or social networking information of this person. \\n\n",
    "6. The first job should have the farest year from current date, the last or current job \n",
    "has the closest to the current date.  From the last and first job you should be able \n",
    "to calculate the total year a person has worked for each job and total years of a person\n",
    "has been worked.\\n\n",
    "    \n",
    "    \n",
    "\"\"\"\n",
    "'''\n",
    "\n",
    "additional_background = \"\"\n",
    "question=\"How long has each candidate been involved with robotics, and what roles have they held in this field?\"\n",
    "step_by_step_decomposition(question, additional_background, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Final Rag - Aggregation answer is:     Candidate 1, Sam C. Yang, has had the following roles in the field of robotics:\n",
    "\n",
    "1. Miso Robotics - Tech. Program Manager from 11/2021 to 09/2022 (duration: 10 months)\n",
    "2. Quasar Science - NPI Program Manager from 6/2020 to 11/2021 (duration: 1 year, 5 months)\n",
    "\n",
    "Candidate 2, Teresa Technical, does not have any specific experience mentioned in the field of robotics.\n",
    "\n",
    "Candidate 3, Daisuke Data Science, does not have any specific experience mentioned in the field of robotics.\n",
    "\n",
    "Final Rag - Aggregation answer is:     Candidate 1, Sam C. Yang, has had the following roles in the field of robotics:\n",
    "\n",
    "1. Miso Robotics - Tech. Program Manager from 11/2021 to 09/2022 (duration: 10 months)\n",
    "2. Quasar Science - NPI Program Manager from 6/2020 to 11/2021 (duration: 1 year, 5 months)\n",
    "\n",
    "Candidate 2, Teresa Technical, does not have any specific experience mentioned in the field of robotics.\n",
    "\n",
    "Candidate 3, Daisuke Data Science, does not have any specific experience mentioned in the field of robotics.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleanup the vectordb\n",
    "def clean_vectorstore():\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    ectorstore = Chroma(\"langchain\", embeddings)\n",
    "    ids = vectorstore.get().get('ids')\n",
    "    len(ids)\n",
    "    print(\"ids \", ids)\n",
    "    for id in ids: \n",
    "        docs = vectorstore.get(id)\n",
    "        print(\"docs \", docs)\n",
    "        print(id)\n",
    "        vectorstore.delete(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_vectorstore()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cpu",
   "language": "python",
   "name": "cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
